{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Information Theory in Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this tutorial on the basics of information theory and its practical application in optimizing experimental design! In this session, we'll delve into the fundamental concepts of information content, entropy, conditional entropy, and mutual information, all within the context of a synthetic working memory experiment.\n",
    "\n",
    "#### Why should you even bother about Information Theory when designing an experiment?\n",
    "\n",
    "Imagine you're designing an experiment to understand how human working memory performs under conditions like the amount of information to remember. How do you choose the amount participants should remember? How do you quantify the information gained from each trial about this cognitive system under study? This is where information theory steps in.\n",
    "\n",
    "Information theory provides a powerful framework for measuring and quantifying uncertainty, information content, and the relationships between variables. In this tutorial, we'll guide you through the basics and show you how to apply these concepts in experimental design.\n",
    "\n",
    "#### Optimizing the experimental design for a working memory experiment\n",
    "\n",
    "Our context for learning will be a synthetic working memory experiment. \n",
    "\n",
    "In this experiment, participants (experimental units) are asked to remember a sequence of the length *L* which contains random numbers. \n",
    "\n",
    "We, as the researchers, have control about the sequence length *L*. This is the factor in our experiment.\n",
    "\n",
    "The participants response will be categorized either being *Correct* (1) or *False* (0).\n",
    "\n",
    "#### Tutorial Overview\n",
    "\n",
    "In this tutorial you will learn to:\n",
    "- Set up a synthetic working memory experiment (remember what you learned in the tutorial about synthetic experiments)\n",
    "- Compute the information content and the entropy of the responses\n",
    "- Choose new factor levels based on our knowledge of the entropy\n",
    "- Compute the mutual information between our model parameters, the factors and the given responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the synthetic experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first section, we are going to set up the synthetic experiment by defining the experimental unit.\n",
    "\n",
    "You can think of each unit being one participant.\n",
    "\n",
    "Remember what you learned in the tutorial about synthetic experimentation where you implemented everything by yourselves.\n",
    "\n",
    "But instead of implementing everything on our own, we just take the already implemented experimental_unit class, a ground truth and a noise method. You can find these elements in the `synthetic.py` file.\n",
    "\n",
    "Import the synthetic file with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.synthetic import experimental_unit, sigmoid_ground_truth, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground truth method synthetic.sigmoid_ground_truth will give you a curve of sigmoidal shape as you know it already from the presented working memory experiment from the lecture.\n",
    "\n",
    "In this case the ground truth does not give directly the response because it just tells you the probability for remembering the sequence correctly for a sequence of sequence length *L* under a certain parameter *p*.\n",
    "\n",
    "We have to wrap it first into a method that transforms these probabilities into a binary response (remember: 0 = false; 1 = correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a wrapper for the ground truth method\n",
    "# this is needed to transform the probability given by the ground truth into a binary response (0: False or 1: Correct)\n",
    "\n",
    "def ground_truth_wrapper(conditions, parameters, *args):\n",
    "    # give a random response based on the probability given by the ground truth\n",
    "    # get a random variable between 0 and \n",
    "    \n",
    "    # make sure that the inputs of the wrapper are same ones as for the ground truth\n",
    "    probability = sigmoid_ground_truth(conditions, parameters)\n",
    "    # define a random variable which we will check against the probability given by the ground truth to decide if we return 1 or 0\n",
    "    random_variable = np.random.uniform(0, 1)\n",
    "    # check if the random variable is smaller than the probability given by the ground truth -> return 1; else return 0\n",
    "    # example: if the probability given by the ground truth is 0.7, we return 1 in 70% of the cases and 0 in 30% of the cases\n",
    "    return (random_variable < probability).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the wrapper and see what it actually does!\n",
    "\n",
    "First, we are going to define the maximum sequence length for our experiment.\n",
    "\n",
    "Then we set up one participant by simply defining a parameter and check the probabilities of answering correctly - in the real world case we can't observe that probability directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = np.arange(1, maximum_sequence_length)  # sequence lengths for which we want to know the probability of correct answer\n",
    "parameters = (7, -1)  # participant parameter\n",
    "\n",
    "# collect the probabilities for each sequence length\n",
    "probs = np.zeros(len(sequence_lengths))\n",
    "for i, l in enumerate(sequence_lengths):\n",
    "    probs[i] = sigmoid_ground_truth(l, parameters)\n",
    "\n",
    "# plot the ground truth probability curve    \n",
    "plt.plot(sequence_lengths, probs, '-o')\n",
    "plt.ylabel('Probability of correct answer')\n",
    "plt.xlabel('Sequence length')\n",
    "plt.title('Probability of this participant answering correctly for a sequence of length L')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does your graph look like? Does it resemble the sigmoidal shape with a 50% probability of answering correctly around the sequence length 7?\n",
    "\n",
    "If not, you may want to adjust your model parameters.\n",
    "\n",
    "Now we are going to take this ground truth and wrap it into the previously defined method that actually returns a binary response based on the response probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 6 # sequence length\n",
    "for i in range(10):\n",
    "    binary_response = ground_truth_wrapper(l, parameters)\n",
    "    print(f'The given response is: {binary_response};\\tThe actual probability of correct answer is: {np.round(sigmoid_ground_truth(l, parameters), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to probability theory, our wrapper should now approximate the correct probabilities for $n_\\text{repitions}\\rightarrow\\inf$.\n",
    "\n",
    "Let's check how our knowledge of the model reflects real circumstances based on the amount of repetitions per condition (sequence length).\n",
    "\n",
    "First, we are going to collect the responses of the defined participant for each sequence length $n_\\text{repitions}$ times.\n",
    "\n",
    "We are going to do $n_\\text{experiments}$ independent repetitions of the experiment to show how the experiment-to-experiment variance and therefore our certainty/confidence about the resulting model changes with the amount of repitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repetitions = 1  # number of repetitions for each sequence length\n",
    "\n",
    "n_experiments = 100  # number of repetitions for the entire experiment to visualize difference in obtained results\n",
    "\n",
    "# do not change the code from this point\n",
    "np.random.seed(42) \n",
    "binary_response = np.zeros((n_experiments, len(sequence_lengths), n_repetitions))\n",
    "found_probs = np.zeros((n_experiments, len(sequence_lengths)))\n",
    "\n",
    "# collect the responses from the wrapper method\n",
    "# we are going to repeat this 10 times for each sequence length\n",
    "for e in range(n_experiments):\n",
    "    for i, l in enumerate(sequence_lengths):\n",
    "        for r in range(n_repetitions):\n",
    "            binary_response[e, i, r] = ground_truth_wrapper(l, parameters)\n",
    "\n",
    "found_probs = np.sum(binary_response, axis=-1)/n_repetitions\n",
    "\n",
    "# plot the ground truth probability curve \n",
    "for e in range(n_experiments):\n",
    "    plt.plot(sequence_lengths, found_probs[e], '--', alpha=0.3, color='tab:orange')\n",
    "plt.plot(sequence_lengths, probs, '-o', color='tab:blue')\n",
    "plt.ylabel('Probability of correct answer')\n",
    "plt.xlabel('Sequence length')\n",
    "plt.title('Probability of this participant answering correctly for a sequence of length L')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like? Change the number of repetitions to see how the found probability approximates the actual one.\n",
    "\n",
    "In the next step we will make a bunch of experimental units with the implemented wrapper function and the set of parameters defined in the beginning.\n",
    "\n",
    "Unlike the last tutorial for synthetic experimentation, we won't need any noise this time because our wrapper gives already a randomized response by generating a random number which is simply checked against the given probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting the data\n",
    "\n",
    "In this section we are going to set up our data collection and generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset parameters\n",
    "\n",
    "# number of experimental units\n",
    "n_units = 20\n",
    "\n",
    "# sequence lengths which we are going to use\n",
    "sequence_lengths = np.arange(1, 100).reshape(-1, 1)\n",
    "\n",
    "# number of repetitions per condition\n",
    "n_repetitions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of parameters which are passed to the experimental_unit class\n",
    "parameter_0 = np.random.normal(7, 0.5, (n_units, 1))\n",
    "parameter_1 = np.random.normal(-1, 0.25, (n_units, 1))\n",
    "parameters = np.concatenate((parameter_0, parameter_1), axis=1)\n",
    "\n",
    "# defining an experimental unit with the wrapper method\n",
    "experimental_units = []\n",
    "for p in parameters:\n",
    "    experimental_units.append(\n",
    "        experimental_unit(\n",
    "            problem_solver=ground_truth_wrapper, \n",
    "            noise=noise, \n",
    "            parameters=p, \n",
    "            noise_level=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f'Number of experimental units in the dataset: {len(experimental_units)}')\n",
    "test_sequence_length = 7\n",
    "if experimental_units[0].step(test_sequence_length)[0] == 1:\n",
    "    print(f'Given a sequence length of {test_sequence_length}, one of the experimental units repeated the sequence successfully')\n",
    "else:\n",
    "    print(f'Given a sequence length of {test_sequence_length}, one of the experimental units failed to repeat the sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your setup look good?\n",
    "\n",
    "If that's the case, great! Then we can collect the data.\n",
    "\n",
    "\n",
    "The dataset is a 4D tensor with the following dimensions:\n",
    "1. number of experimental units\n",
    "2. number of sequence lengths\n",
    "3. number of repetitions\n",
    "4. features -> unit_id, sequence_length, observation\n",
    "\n",
    "You can use the `generate_dataset` method, which you should already know from the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.synthetic import generate_dataset\n",
    "\n",
    "dataset, dataset_flat = generate_dataset(experimental_units, sequence_lengths, n_repetitions, True)\n",
    "\n",
    "conditions = dataset[:, :, :, 1]\n",
    "observations = dataset[:, :, :, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean of the observations to get the probability for answering correctly for each sequence_length and for each participant\n",
    "probability = np.mean(observations, axis=-1, keepdims=True)\n",
    "\n",
    "# plot the ground truth probability curve \n",
    "for i, unit in enumerate(probability):\n",
    "    plt.plot(conditions[i, :, 0], unit, '-', alpha=0.1, color='tab:orange')\n",
    "# plt.plot(sequence_lengths, probs, '-o', color='tab:blue')\n",
    "plt.ylabel('Probability of correct answer')\n",
    "plt.xlabel('Sequence length')\n",
    "plt.title('Probability of each participant answering correctly\\nfor each sequence length\\n(averaged across repitions)')\n",
    "plt.show()\n",
    "\n",
    "print(f'The shape of the dataset is: {dataset.shape}')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look good? \n",
    "\n",
    "Then you have successfully finished the experimental setup and we can move on to the application of the information theoretic principles you learned about in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied information theory for experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will apply and compute the information theoretic concepts you learned about in the lecture.\n",
    "\n",
    "This includes information content, entropy, conditional entropy and mutual information.\n",
    "\n",
    "You will learn about:\n",
    "- the information content of obtained samples\n",
    "- the overall entropy of the whole dataset\n",
    "- the conditional entropy i.e., the remaining entropy of the responses when the condition is given\n",
    "- the mutual information between the respones, the sequence lengths and a model which we are going to set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information content\n",
    "\n",
    "In this section we are going to compute the information content of a few samples first in general and then based on the sequence length.\n",
    "\n",
    "To do so, we have to compute the marginal probability for answering correctly and wrong $p(x)$ and then also the conditional probability $p(x|l)$ first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the marginal probability of a correct answer\n",
    "# since we have a binary response, we can compute the marginal probability by taking the mean of the dataset\n",
    "marginal_prob = np.mean(observations)\n",
    "\n",
    "print(f'The marginal probability of a correct answer is: {marginal_prob}')\n",
    "\n",
    "# compute the conditional probability of a correct answer given a sequence length\n",
    "# we can do that by computing the mean of the dataset \n",
    "# first along the repition dimension -> This gives us the mean response given by each experimental unit for each sequence length\n",
    "# second along the experimental unit dimension -> This gives us the mean response for each sequence length over all experimental units\n",
    "conditional_prob = np.mean(observations, axis=2)\n",
    "conditional_prob = np.mean(conditional_prob, axis=0)\n",
    "\n",
    "print('The conditional probability of a correct answer given a sequence length is:')\n",
    "for i, l in enumerate(sequence_lengths):\n",
    "    print(f'{l}: {np.round(conditional_prob[i], 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compute the information content of the possible responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the overall information content for correct and wrong answers\n",
    "# use the computed marginal probability for that\n",
    "information_content_correct = -np.log2(marginal_prob)\n",
    "print(f'The information content of a correct answer is: {information_content_correct}')\n",
    "\n",
    "information_content_wrong = -np.log2(1 - marginal_prob)\n",
    "print(f'The information content of a wrong answer is: {information_content_wrong}')\n",
    "\n",
    "# compute the conditional information content for correct and wrong answers\n",
    "# use the computed conditional probabilities for that\n",
    "conditional_information_content_correct = -np.log2(conditional_prob)\n",
    "conditional_information_content_wrong = -np.log2(1 - conditional_prob)\n",
    "\n",
    "print('The information content of correct and wrong answers given a sequence length are:')\n",
    "print('Length\\tCorrect\\tWrong')\n",
    "for i, l in enumerate(sequence_lengths):\n",
    "    print(f'{l}\\t{np.round(conditional_information_content_correct[i], 2)}\\t{np.round(conditional_information_content_wrong[i], 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the information contents like you would have them expected to be?\n",
    "\n",
    "You should find that the probability of an observation is negatively correlated with its information content.\n",
    "\n",
    "Further, according to the response probabilities per sequence length, you should find that the shorter the presented sequence is, the higher is the information content of a wrong response and vice versa.\n",
    "\n",
    "**What could this mean for our experimental setting? In which areas would you rather sample?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and conditional entropy\n",
    "\n",
    "Here we are going to compute the entropy of the whole dataset, which is the expected information content per response.\n",
    "\n",
    "To do so, we have to compute the average information content of each response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the entropy of the response distribution\n",
    "# use the formula from the lecture\n",
    "entropy = -marginal_prob * np.log2(marginal_prob) - (1 - marginal_prob) * np.log2(1 - marginal_prob)\n",
    "print(f'The entropy of the response distribution is: {entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this result tell you about the distribution and the expected information content?\n",
    "\n",
    "In the binary case, an entropy $H(X)\\neq 1$ indicates that the responses have an unbalanced probability and that the more likely one occures more frequently.\n",
    "\n",
    "If the entropy is very unbalanced, that might be a hint for adapting our sampling strategy so we gather more samples of the less likely response.\n",
    "\n",
    "Let's compute now the conditional entropy and check what this one tells us about our dataset.\n",
    "\n",
    "Remember the formula from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the conditional entropy as the sum of the conditional information content\n",
    "\n",
    "# first, compute the joint probability of the response and the sequence length for each sample\n",
    "# we can do that by finding the amount of occurrences of each combination of response and sequence length \n",
    "# and divide it by the total number of samples\n",
    "joint_prob = np.zeros((len(sequence_lengths), 2))\n",
    "\n",
    "for i, l in enumerate(sequence_lengths):\n",
    "    joint_prob[i, 0] = np.sum(observations[:, i, :] == 0) / observations.size\n",
    "    joint_prob[i, 1] = np.sum(observations[:, i, :] == 1) / observations.size\n",
    "\n",
    "# now we are going to compute the conditional probability of the response given the sequence length\n",
    "# we can do that by applying bayes rule: p(x, y) = p(x)*(y|x) = p(y)*(x|y)\n",
    "prob_l = 1/len(sequence_lengths)\n",
    "prob_x_given_l = np.zeros_like(joint_prob)\n",
    "prob_x_given_l = joint_prob / prob_l \n",
    "\n",
    "# compute the conditional entropy as the sum of the conditional information content\n",
    "# remember: in the case of a probability of 0, the log(0) would be -inf, which is not defined\n",
    "# instead we can set the conditional entropy to 0 since 0*log(0) = 0\n",
    "conditional_entropy = np.zeros_like(joint_prob)\n",
    "for x in range(conditional_entropy.shape[1]):\n",
    "    # correct and wrong\n",
    "    for y in range(conditional_entropy.shape[0]):\n",
    "        # sequence lengths\n",
    "        if prob_x_given_l[y, x] == 0:\n",
    "            conditional_entropy[y, x] = 0\n",
    "        else:\n",
    "            conditional_entropy[y, x] = joint_prob[y, x] * np.log2(prob_x_given_l[y, x])\n",
    "conditional_entropy = -np.sum(conditional_entropy)\n",
    "\n",
    "print(f'The conditional entropy for the whole dataset is: {conditional_entropy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this result tell us about our experiment and the collected data?\n",
    "\n",
    "Remember that the conditional entropy tells us the remaining entropy of the responses given the conditions, which in our case are the sequence lengths.\n",
    "\n",
    "Compare the overall response entropy with the conditional entropy. Did it change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information\n",
    "\n",
    "In the last section you computed the conditional entropy, which is an indicator for remaining entropy of one random variable $X$ given another one $Y$.\n",
    "\n",
    "Do you remember the Venn diagram from the lecture?\n",
    "\n",
    "The mutual information $I(X,Y)$ was also computed with the conditional entropy and the overall entropy with a simple formula.\n",
    "Compute the mutual information of the response and the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_response_length = entropy - conditional_entropy\n",
    "print(f'The mutual information between response and sequence length is: {mutual_information_response_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How high is the mutual information compared to the overall entropy?\n",
    "\n",
    "Can you think of a way to alter the mutual information? We could for example add some noise to the response data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a notion of mystery to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are going to change our observations synthetically by flipping some of the bits and therefore add an unknown yet significant influence. \n",
    "\n",
    "This step simulates how the information characteristics change, when our chosen factors do not cover the cause-effect range to a satisfying degree.\n",
    "\n",
    "You can do so by simply flipping the observations with a specified probability. Flipping means that 0 will become 1 and vice versa.\n",
    "\n",
    "After modifying the dataset, re-run the previous cells from the information theory section and observe how the information characteristics change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_flip = 0.3  # probability of flipping a response\n",
    "\n",
    "idx = np.random.randint(0, observations.size, int(observations.size * prob_flip))  # get random indexes for the dataset\n",
    "idx = np.unravel_index(idx, observations.shape)  # convert the indexes to the shape of the dataset\n",
    "\n",
    "# we flip the responses for these indexes\n",
    "observations[idx] = 1 - observations[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What could be limiting factors to our experiment?\n",
    "\n",
    "How could the remaining high conditional entropy/low mutual information be explained?\n",
    "\n",
    "Is our sample size the limiting factor or the experiment in general?\n",
    "\n",
    "### Is this approach applicable to our response time experiment?\n",
    "\n",
    "Recall the used equations.\n",
    "\n",
    "How is the response time experiment different to the working memory experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've successfully finished the tutorial on the basics of information theory and how to apply them in the cognitive experimental context!\n",
    "\n",
    "This was still a rather simple example regarding that our data had only one factor and the responses were binary.\n",
    "\n",
    "When we are working e.g. with continuous variables things become way more complicated.\n",
    "\n",
    "But for this case, there are already some popular libraries for optimal experimenting design and active learning which we are going to use in the next tutorial!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
