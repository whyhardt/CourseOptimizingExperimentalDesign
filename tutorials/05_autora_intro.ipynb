{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AutoRA & Closed-Loop Experimentation\n",
    "\n",
    "Welcome to the AutoRA framework! In this tutorial, you'll learn how to build automated closed-loop experiments that intelligently select which conditions to test next.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the AutoRA workflow (State → Experimentalist → Runner → Theorist)\n",
    "- Build a simple closed-loop experiment\n",
    "- Compare different experimentalist strategies (grid vs. random)\n",
    "- Evaluate experimental design efficiency\n",
    "\n",
    "## Why Closed-Loop Experimentation?\n",
    "\n",
    "Traditional experiments:\n",
    "1. Design all conditions upfront\n",
    "2. Run all trials\n",
    "3. Analyze data\n",
    "4. (Maybe) design follow-up study\n",
    "\n",
    "Closed-loop experiments:\n",
    "1. Start with small set of conditions\n",
    "2. Run trials → Fit model → Identify informative conditions\n",
    "3. Repeat step 2 until budget exhausted\n",
    "4. Result: More efficient use of resources!\n",
    "\n",
    "## The AutoRA Framework\n",
    "\n",
    "AutoRA consists of three main components:\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│ Experimentalist │ ← Proposes new conditions to test\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│ Experiment      │ ← Runs experiments, collects data\n",
    "│ Runner          │\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "┌─────────────────┐\n",
    "│ Theorist        │ ← Builds model to explain data\n",
    "└────────┬────────┘\n",
    "         ↓\n",
    "  (Loop back to Experimentalist)\n",
    "```\n",
    "\n",
    "All components communicate via a **State** object that contains:\n",
    "- Variables (independent/dependent)\n",
    "- Conditions (to be tested)\n",
    "- Experiment data (collected so far)\n",
    "- Models (current best explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add project folder to path\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Experiment Runner\n",
    "\n",
    "First, let's import our 2AFC experiment. The experiment runner wraps our synthetic experiment into a format AutoRA can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.synthetic import twoafc\n",
    "\n",
    "# Define participant parameters\n",
    "n_units = 100\n",
    "parameters = np.random.normal(1, 0.5, (n_units, 2))\n",
    "parameters = np.where(parameters < 0, 0, parameters)\n",
    "\n",
    "# Create experiment\n",
    "experiment = twoafc(parameters, resolution=10)\n",
    "\n",
    "# Let's see what it looks like\n",
    "print(\"Experiment created!\")\n",
    "print(f\"Number of participants: {n_units}\")\n",
    "print(f\"\\nIndependent Variables: {[iv.name for iv in experiment.variables.independent_variables]}\")\n",
    "print(f\"Dependent Variables: {[dv.name for dv in experiment.variables.dependent_variables]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a single trial to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment with one condition\n",
    "test_conditions = np.array([[0, 0.5, 0.5]])  # [participant_id, ratio, scatteredness]\n",
    "result = experiment.run(test_conditions, random_state=42)\n",
    "\n",
    "print(\"Experimental result:\")\n",
    "print(result)\n",
    "print(f\"\\nParticipant {int(result['participant_id'][0])} with ratio={result['ratio'][0]:.2f} and scatteredness={result['scatteredness'][0]:.2f} had response time={result['response_time'][0]:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the underlying ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding States\n",
    "\n",
    "The State is the central data structure in AutoRA. It keeps track of everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.state import StandardState\n",
    "\n",
    "# Get variable names\n",
    "iv_names = [iv.name for iv in experiment.variables.independent_variables]\n",
    "dv_names = [dv.name for dv in experiment.variables.dependent_variables]\n",
    "\n",
    "# Initialize with empty state\n",
    "state = StandardState(\n",
    "    variables=experiment.variables,\n",
    "    conditions=pd.DataFrame(columns=iv_names),  # No conditions yet\n",
    "    experiment_data=pd.DataFrame(columns=iv_names + dv_names),  # No data yet\n",
    "    models=[]  # No model yet\n",
    ")\n",
    "\n",
    "print(\"State created!\")\n",
    "print(f\"Variables: {len(state.variables.independent_variables)} IVs, {len(state.variables.dependent_variables)} DVs\")\n",
    "print(f\"Conditions: {len(state.conditions)} rows\")\n",
    "print(f\"Experiment data: {len(state.experiment_data)} rows\")\n",
    "print(f\"Models: {len(state.models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First Experimentalist - Grid Sampler\n",
    "\n",
    "The experimentalist proposes which conditions to test. Let's start with a simple one: **grid sampling**.\n",
    "\n",
    "Grid sampling creates evenly-spaced points across the design space - just like a factorial design!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.grid import grid_pool\n",
    "from autora.state import on_state\n",
    "\n",
    "# Wrap grid_pool to work with states\n",
    "experimentalist_grid = on_state(grid_pool, output=['conditions'])\n",
    "\n",
    "# Generate grid conditions\n",
    "# sample_all=['participant_id'] means: generate conditions for ALL participants\n",
    "state = experimentalist_grid(state, num_samples=5, sample_all=['participant_id'])\n",
    "\n",
    "print(f\"Generated {len(state.conditions)} conditions!\")\n",
    "print(\"\\nFirst few conditions:\")\n",
    "print(state.conditions.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the proposed conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for one participant\n",
    "participant_0 = state.conditions[state.conditions['participant_id'] == 0]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(participant_0['ratio'], participant_0['scatteredness'], s=100, alpha=0.6)\n",
    "plt.xlabel('Ratio', fontsize=12)\n",
    "plt.ylabel('Scatteredness', fontsize=12)\n",
    "plt.title('Grid Sampling: Proposed Conditions for Participant 0', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Grid sampler created {len(participant_0)} evenly-spaced conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running the Experiment\n",
    "\n",
    "Now let's collect data for these conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap experiment runner to work with states\n",
    "experiment_runner = on_state(experiment.run, output=['experiment_data'])\n",
    "\n",
    "# Run experiment\n",
    "state = experiment_runner(state, added_noise=0.0, random_state=42)\n",
    "\n",
    "print(f\"Collected {len(state.experiment_data)} observations!\")\n",
    "print(\"\\nFirst few observations:\")\n",
    "print(state.experiment_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training a Model (Theorist)\n",
    "\n",
    "Now we fit a model to explain the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.regressors import FFN, FFNRegressor\n",
    "from autora.state import estimator_on_state\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create model\n",
    "model = FFNRegressor(FFN(n_units, 2), max_epochs=50, lr=0.1, verbose=False)\n",
    "\n",
    "# Wrap to work with states\n",
    "theorist = estimator_on_state(model)\n",
    "\n",
    "# Store initial model in state\n",
    "state.models = [model]\n",
    "\n",
    "# Train!\n",
    "state = theorist(state)\n",
    "\n",
    "print(\"Model trained!\")\n",
    "print(f\"State now contains {len(state.models)} model(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X = state.experiment_data[iv_names].values\n",
    "y_true = state.experiment_data[dv_names].values\n",
    "y_pred = state.models[0].predict(X)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Model MSE: {mse:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "experiment.plotter(state.models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Closing the Loop!\n",
    "\n",
    "Now let's run multiple cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset state\n",
    "state_grid = StandardState(\n",
    "    variables=experiment.variables,\n",
    "    conditions=pd.DataFrame(columns=iv_names),\n",
    "    experiment_data=pd.DataFrame(columns=iv_names + dv_names),\n",
    "    models=[FFNRegressor(FFN(n_units, 2), max_epochs=50, lr=0.1, verbose=False)]\n",
    ")\n",
    "\n",
    "# Run 5 cycles\n",
    "n_cycles = 5\n",
    "mse_history_grid = []\n",
    "\n",
    "for cycle in range(n_cycles):\n",
    "    print(f\"\\nCycle {cycle + 1}/{n_cycles}\")\n",
    "    \n",
    "    # 1. Propose conditions\n",
    "    state_grid = experimentalist_grid(state_grid, num_samples=2, sample_all=['participant_id'])\n",
    "    \n",
    "    # 2. Run experiment\n",
    "    state_grid = experiment_runner(state_grid, added_noise=0.0, random_state=42+cycle)\n",
    "    \n",
    "    # 3. Train model\n",
    "    state_grid = theorist(state_grid)\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    X = state_grid.experiment_data[iv_names].values\n",
    "    y_true = state_grid.experiment_data[dv_names].values\n",
    "    y_pred = state_grid.models[0].predict(X)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mse_history_grid.append(mse)\n",
    "    \n",
    "    print(f\"  Total samples: {len(state_grid.experiment_data)}\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "\n",
    "print(\"\\nClosed-loop experiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Experimentalists\n",
    "\n",
    "Let's compare grid sampling vs. random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.random import random_sample\n",
    "\n",
    "# Random experimentalist\n",
    "experimentalist_random = on_state(random_sample, output=['conditions'])\n",
    "\n",
    "# Reset state\n",
    "state_random = StandardState(\n",
    "    variables=experiment.variables,\n",
    "    conditions=pd.DataFrame(columns=iv_names),\n",
    "    experiment_data=pd.DataFrame(columns=iv_names + dv_names),\n",
    "    models=[FFNRegressor(FFN(n_units, 2), max_epochs=50, lr=0.1, verbose=False)]\n",
    ")\n",
    "\n",
    "# Run same number of cycles\n",
    "mse_history_random = []\n",
    "\n",
    "for cycle in range(n_cycles):\n",
    "    state_random = experimentalist_random(state_random, num_samples=2, random_state=42+cycle, sample_all=['participant_id'])\n",
    "    state_random = experiment_runner(state_random, added_noise=0.0, random_state=42+cycle)\n",
    "    state_random = theorist(state_random)\n",
    "    \n",
    "    X = state_random.experiment_data[iv_names].values\n",
    "    y_true = state_random.experiment_data[dv_names].values\n",
    "    y_pred = state_random.models[0].predict(X)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mse_history_random.append(mse)\n",
    "\n",
    "print(\"Comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_cycles+1), mse_history_grid, 'o-', label='Grid Sampling', linewidth=2)\n",
    "plt.plot(range(1, n_cycles+1), mse_history_random, 's-', label='Random Sampling', linewidth=2)\n",
    "plt.xlabel('Cycle', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "plt.title('Experimentalist Comparison: Grid vs Random', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal MSE:\")\n",
    "print(f\"  Grid: {mse_history_grid[-1]:.4f}\")\n",
    "print(f\"  Random: {mse_history_random[-1]:.4f}\")\n",
    "print(f\"\\nGrid sampling is {(mse_history_random[-1]/mse_history_grid[-1] - 1)*100:.1f}% better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize where each strategy sampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get conditions for participant 0\n",
    "grid_conds = state_grid.experiment_data[state_grid.experiment_data['participant_id'] == 0]\n",
    "random_conds = state_random.experiment_data[state_random.experiment_data['participant_id'] == 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Grid\n",
    "axes[0].scatter(grid_conds['ratio'], grid_conds['scatteredness'], s=100, alpha=0.6, c=range(len(grid_conds)), cmap='viridis')\n",
    "axes[0].set_xlabel('Ratio', fontsize=12)\n",
    "axes[0].set_ylabel('Scatteredness', fontsize=12)\n",
    "axes[0].set_title('Grid Sampling (Participant 0)', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlim(-0.1, 1.1)\n",
    "axes[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Random\n",
    "axes[1].scatter(random_conds['ratio'], random_conds['scatteredness'], s=100, alpha=0.6, c=range(len(random_conds)), cmap='viridis')\n",
    "axes[1].set_xlabel('Ratio', fontsize=12)\n",
    "axes[1].set_ylabel('Scatteredness', fontsize=12)\n",
    "axes[1].set_title('Random Sampling (Participant 0)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(-0.1, 1.1)\n",
    "axes[1].set_ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Grid: {len(grid_conds)} samples, evenly distributed\")\n",
    "print(f\"Random: {len(random_conds)} samples, clustered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. ✅ **AutoRA Workflow**: State → Experimentalist → Runner → Theorist\n",
    "2. ✅ **State Management**: Central data structure for closed-loop experiments\n",
    "3. ✅ **Experimentalists**: Grid vs. Random sampling\n",
    "4. ✅ **Closed-Loop**: Iterative refinement improves model quality\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next tutorials, you'll learn:\n",
    "- **Information theory**: Why some samples are more valuable than others\n",
    "- **Uncertainty sampling**: Intelligent selection based on model uncertainty\n",
    "- **Disagreement sampling**: Using model ensembles for even smarter selection\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Modify sample size**: Try `num_samples=1` vs `num_samples=10` per cycle. How does this affect convergence?\n",
    "\n",
    "2. **Add noise**: Change `added_noise=0.0` to `added_noise=0.5`. How do the strategies compare now?\n",
    "\n",
    "3. **More participants**: Reduce `n_units` to 10. Does this change which strategy is better?\n",
    "\n",
    "4. **Cycle count**: Run 20 cycles instead of 5. Do the strategies converge to similar performance?\n",
    "\n",
    "5. **Latin Hypercube**: Try using `from scipy.stats.qmc import LatinHypercube` as an experimentalist. How does it compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the AutoRA introduction tutorial! You now understand the fundamentals of closed-loop experimentation and can build your own automated experiment workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
