{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Active Learning with modAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is based on the official documentation. For more information click [here](https://modal-python.readthedocs.io/en/latest/content/overview/modAL-in-a-nutshell.html) to get to the official website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about introducing you to the concept of active learning through first steps with the *modAL* package.\n",
    "\n",
    "Here you will learn to:\n",
    "- write a simple active learning sampling strategy\n",
    "- utilize the defined sampling strategy as part of the *modAL* package\n",
    "\n",
    "**modAL** is an active learning framework for Python3, designed with modularity, flexibility and extensibility in mind. Built on top of scikit-learn, it allows you to rapidly create active learning workflows with nearly complete freedom. What is more, you can easily replace parts with your custom built solutions, allowing you to design novel algorithms with ease.\n",
    "\n",
    "With the recent explosion of available data, you have can have millions of unlabelled examples with a high cost to obtain labels. For instance, when trying to predict the sentiment of tweets, obtaining a training set can require immense manual labour. But worry not, active learning comes to the rescue! In general, AL is a framework allowing you to increase classification performance by intelligently querying you to label the most informative instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install modAL you can use this command which will install the package directly from source:\n",
    "\n",
    "`%pip install git+https://github.com/modAL-python/modAL.git`\n",
    "\n",
    "(Remove `%` if you are running this command from your terminal or from a .py-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install git+https://github.com/modAL-python/modAL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modAL in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active learning with a scikit-learn classifier, for instance RandomForestClassifier, can be as simple as the following.\n",
    "\n",
    "```\n",
    "from modAL.models import ActiveLearner\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initializing the learner\n",
    "learner = ActiveLearner(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    X_training=X_training, y_training=y_training\n",
    ")\n",
    "\n",
    "# query for labels\n",
    "query_idx, query_inst = learner.query(X_pool)\n",
    "\n",
    "# ...obtaining new labels from the Oracle...\n",
    "\n",
    "# supply label for queried instance\n",
    "learner.teach(X_pool[query_idx], y_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example with active regression\n",
    "\n",
    "To see modAL in real action, letâ€™s consider an active regression problem with Gaussian processes! In this example, we shall try to learn a noisy sine function.\n",
    "\n",
    "Let's define our dataset with the inputs `X` and the outputs `Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# training data\n",
    "value_range = (0, 20)\n",
    "x_allowed = np.linspace(*value_range, 1000).reshape(-1, 1)\n",
    "ground_truth = lambda x: np.sin(x) + np.random.normal(scale=0.3, size=x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For active learning, we shall define a custom query strategy tailored to Gaussian processes. In a nutshell, a *query stategy* in modAL is a function taking (at least) two arguments (an estimator object and a pool of examples), outputting the index of the queried instance and the instance itself. In our case, the arguments are `regressor` and `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP_regression_std(regressor, X, n_instances=1):\n",
    "    \n",
    "    _, std = regressor.predict(X.reshape(-1, 1), return_std=True)  # for a Gaussian process estimator, we get the standard deviation easily\n",
    "    # get the n_instances indices with the highest standard deviation\n",
    "    query_idx = np.argsort(std)[-n_instances:]    \n",
    "    return query_idx, X[query_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the query strategy and the data, the active learner can be initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, RBF\n",
    "\n",
    "n_initial = 5  # initial amount of random samples\n",
    "\n",
    "# get initial training data - random samples\n",
    "# initial_idx = np.random.choice(range(len(x_training)), size=n_initial, replace=False)  # draw random initial indices\n",
    "# x_training_init, y_training_init = x_training[initial_idx], y_training[initial_idx]  # set the initial training data\n",
    "x_initial = np.random.uniform(*value_range, (n_initial, 1))\n",
    "y_initial = ground_truth(x_initial)\n",
    "\n",
    "# define a kernel for the Gaussian process regressor\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n",
    "         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "\n",
    "# initialize the regressor and train it with the initial training data\n",
    "regressor = ActiveLearner(\n",
    "    estimator=GaussianProcessRegressor(kernel=kernel),\n",
    "    query_strategy=GP_regression_std,\n",
    "    X_training=x_initial, y_training=y_initial\n",
    ")\n",
    "\n",
    "x_querried = np.array(x_initial)\n",
    "y_querried = np.array(y_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well the estimator is fitted by first defining a plotting function and then calling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# plotting method which we can reuse later\n",
    "def plotting_method(regressor, x, x_true=None, y_true=None, mse=None, x_new=None, x_querried=None, y_querried=None, return_std_available=True):\n",
    "    if return_std_available:\n",
    "        y, y_std = regressor.predict(x, return_std=True)\n",
    "    else:\n",
    "        y = regressor.predict(x).reshape(-1,)\n",
    "        y_std = np.zeros_like(y)\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x[:, -1], y, label='Prediction')\n",
    "    plt.fill_between(x[:, -1], y - y_std, y + y_std, alpha=0.2)\n",
    "    if x_true is not None and y_true is not None:\n",
    "        plt.scatter(x_true, y_true, c='grey', s=20, label='Training samples')\n",
    "    if x_new is not None:\n",
    "        # make a vertical line to indicate the new training sample\n",
    "        if isinstance(x_new, (int, float)):\n",
    "            x_new = [x_new]\n",
    "        for x in x_new:\n",
    "            plt.axvline(x=x, c='r', linestyle='--')\n",
    "    if x_querried is not None and y_querried is not None:\n",
    "        plt.plot(x_querried, y_querried, 'x', color='red', label='Querried samples')\n",
    "    if mse is not None:\n",
    "        plt.title(f'Prediction over the whole grid (mse={np.round(mse, 4)})')\n",
    "    else:\n",
    "        plt.title('Prediction')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get querried y-values\n",
    "plotting_method(regressor, x_allowed, x_new=x_initial, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue band enveloping the regressor represents the standard deviation of the Gaussian process at the given point. \n",
    "\n",
    "Now we are ready to do active learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries = 1\n",
    "\n",
    "_, x_new = regressor.query(x_allowed, n_instances=n_queries)  # get the query points from the regressor which are expected to be the most informative\n",
    "x_new = x_new.reshape(-1, 1)\n",
    "y_new = ground_truth(x_new)\n",
    "\n",
    "regressor.teach(x_new, y_new)  # fit the model with the query points\n",
    "\n",
    "x_querried = np.concatenate((x_querried, x_new))\n",
    "y_querried = np.concatenate((y_querried, y_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the regressor benefited from the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted response and the standard deviation\n",
    "y_pred, y_std = regressor.predict(x_allowed, return_std=True)\n",
    "\n",
    "plotting_method(regressor, x_allowed, x_new=x_new, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look already any better?\n",
    "\n",
    "What happened from the initial plot to the second one?\n",
    "\n",
    "Let's iterate a few more times and check the result after each iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    x_new = regressor.query(x_allowed, n_instances=n_queries)[1].reshape(-1, 1)  # get the query points from the regressor which are expected to be the most informative\n",
    "    y_new = ground_truth(x_new)\n",
    "    regressor.teach(x_new, y_new)  # fit the model with the query points\n",
    "    \n",
    "    x_querried = np.concatenate((x_querried, x_new))\n",
    "    y_querried = np.concatenate((y_querried, y_new))\n",
    "    \n",
    "    # get the predicted response and the standard deviation\n",
    "    y_pred, y_std = regressor.predict(x_allowed, return_std=True)\n",
    "    # y_pred, y_std = y_pred.ravel(), y_std.ravel()\n",
    "\n",
    "    plotting_method(regressor, x_allowed, x_new=x_new, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the same model with a random sampling method\n",
    "\n",
    "Now we implement a simple random sampling method and give it directly the amount of training samples which we gave to the active learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_random_points = n_initial + (1+iterations)*n_queries\n",
    "\n",
    "# get a training set of random points and evaluate the regression\n",
    "x_random = np.random.uniform(*value_range, (n_random_points, 1))\n",
    "y_random = ground_truth(x_random)\n",
    "\n",
    "regressor_rnd = GaussianProcessRegressor(kernel=kernel)  # initialize the same regressor as for the active learner\n",
    "regressor_rnd.fit(x_random, y_random)  # fit the regressor to the random data\n",
    "\n",
    "plotting_method(regressor_rnd, x_allowed, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compare these two models against the true data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = x_allowed\n",
    "y_true = ground_truth(x_true)\n",
    "\n",
    "y_al = regressor.predict(x_true)\n",
    "y_rnd = regressor_rnd.predict(x_true)\n",
    "\n",
    "mse_al = mean_squared_error(y_true, y_al)\n",
    "mse_rnd = mean_squared_error(y_true, y_rnd)\n",
    "\n",
    "plotting_method(regressor_rnd, x_allowed, x_true=x_true, y_true=y_true, mse=mse_rnd, x_querried=x_querried, y_querried=y_querried)\n",
    "plotting_method(regressor, x_allowed, x_true=x_true, y_true=y_true, mse=mse_al, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the estimator with random sampling perform compared to active learning?\n",
    "\n",
    "Can you explain the results by looking step-by-step at the chosen samples by the active learning algorithm?\n",
    "\n",
    "Try and increase the noise bit-by-bit e.g., in steps of 0.2 and re-run the code.\n",
    "\n",
    "What happens hereby?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning with the FFNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will replace the `GaussianProcessRegressor` with our `FFNRegressor`.\n",
    "\n",
    "Unlike the `GaussianProcessRegressor`, the `FFNRegressor` does not provide any estimation of the standard deviation, i.e. we do not get an uncertainty measure. Fortunately, modAL has a solution for that!\n",
    "\n",
    "This solution is called **Committee** or **Ensemble**. An ensemble carries several initializations of the same type of model.\n",
    "The uncertainty is estimated based on the degree of **Disagreement** between the models for the same conditions, i.e. if we provide the ensemble with the two conditions `X=1` and `X=2` we would get e.g. `(0.8, 0.34, 0.67, 0.98) = ensemble(X=1)` and `(1.31, 1.34, 1.47, 1.28) = ensemble(X=2)`, respectively. By computing the variance of the predictions, we can tell for which condition the ensemble is more uncertain - in this case for condition `X=1`. Therefore, we would querry a new sample at `X=1`.\n",
    "\n",
    "The models are trained in parallel without *sharing any model-internal information*. When training these models in parallel you should create sub-datasets for models by e.g. **Bootstrapping** with or without replacement. This generates slightly different datasets for each of the models and prevents them from converging too fast to the same predictions for all conditions.\n",
    "\n",
    "In modAL these things are all implemented in the class `CommitteeRegressor` and the querry strategy `max_std_sampling`.\n",
    "\n",
    "Bootstrapping can be done with e.g. the sklearn method `sklearn.utils.resample`.\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from modAL.models import CommitteeRegressor\n",
    "from modAL.disagreement import max_std_sampling\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)\n",
    "    \n",
    "from resources.regressors import FFN, FFNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of models in the ensemble\n",
    "n_ensemble = 8\n",
    "\n",
    "# number of initial samples\n",
    "n_initial = 50\n",
    "\n",
    "# number of querried samples per active learning iteration\n",
    "n_querries = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial samples\n",
    "x_initial = np.random.uniform(*value_range, n_initial).reshape(-1, 1)\n",
    "y_initial = ground_truth(x_initial)\n",
    "\n",
    "x_querried = np.array(x_initial)\n",
    "y_querried = np.array(y_initial)\n",
    "\n",
    "# Let's gather the subdatasets\n",
    "samples_intial_ens = [resample(x_initial, y_initial) for _ in range(n_ensemble)]\n",
    "\n",
    "# take a look at the bootstrapped datasets\n",
    "# print(samples_intial_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we have to initialize the single active learners and collect them\n",
    "# in this case each learner will be trained on only one sample\n",
    "learner_list = []\n",
    "for idx_learner in range(n_ensemble):\n",
    "    x_initial_ens = samples_intial_ens[idx_learner][0]\n",
    "    y_initial_ens = samples_intial_ens[idx_learner][1]\n",
    "    \n",
    "    # add an artificial unit_id --> otherwise FFN does not work\n",
    "    unit_id = np.zeros((x_initial_ens.shape[0], 1))\n",
    "    x_initial_ens = np.concatenate((unit_id, x_initial_ens), axis=-1) \n",
    "    \n",
    "    learner_list.append(\n",
    "        ActiveLearner(\n",
    "            estimator=FFNRegressor(FFN(n_units=1, n_conditions=1, hidden_size=16, embedding_size=1), max_epochs=100, lr=0.01, verbose=False),\n",
    "            X_training=x_initial_ens, y_training=y_initial_ens,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at what the initialized FFNs are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ffn in learner_list:\n",
    "    unit_id = np.zeros((x_allowed.shape[0], 1))\n",
    "    x = np.concatenate((unit_id, x_allowed), axis=-1)\n",
    "    plotting_method(ffn, x, x_querried=x_querried, y_querried=y_querried, return_std_available=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the `CommitteeRegressor` and take a look at the joined prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the Committee\n",
    "committee = CommitteeRegressor(\n",
    "    learner_list=learner_list,\n",
    "    query_strategy=max_std_sampling\n",
    ")\n",
    "\n",
    "print(f'Number of learners in the committee: {len(committee)}')\n",
    "\n",
    "unit_id = np.zeros((x_allowed.shape[0], 1))\n",
    "x = np.concatenate((unit_id, x_allowed), axis=-1)\n",
    "plotting_method(committee, x, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After everything is set up, we can now start with our active learning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_queries = 1\n",
    "\n",
    "unit_id = np.zeros((x_allowed.shape[0], 1))\n",
    "x = np.concatenate((unit_id, x_allowed), axis=-1)\n",
    "_, x_new = committee.query(x, n_instances=n_queries, random_tie_break=True)  # get the query points from the regressor which are expected to be the most informative\n",
    "x_new = x_new.reshape(-1, 1)\n",
    "y_new = ground_truth(x_new)\n",
    "\n",
    "unit_id = np.zeros((x_new.shape[0], 1))\n",
    "x_teach = np.concatenate((unit_id, x_new), axis=-1)\n",
    "\n",
    "committee.teach(x_teach, y_new)  # fit the model with the query points\n",
    "\n",
    "x_querried = np.concatenate((x_querried, x_new))\n",
    "y_querried = np.concatenate((y_querried, y_new))\n",
    "\n",
    "# get the predicted response and the standard deviation\n",
    "plotting_method(committee, x, x_new=x_new, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it look like? Let's give it some more iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "\n",
    "for _ in range(iterations):\n",
    "    unit_id = np.zeros((x_allowed.shape[0], 1))\n",
    "    x = np.concatenate((unit_id, x_allowed), axis=-1)\n",
    "    _, x_new = committee.query(x, n_instances=n_querries, random_tie_break=True)  # get the query points from the regressor which are expected to be the most informative\n",
    "    x_new = x_new.reshape(-1, 1)\n",
    "    y_new = ground_truth(x_new)\n",
    "\n",
    "    unit_id = np.zeros((x_new.shape[0], 1))\n",
    "    x_teach = np.concatenate((unit_id, x_new), axis=-1)\n",
    "    committee.teach(x_teach, y_new)  # fit the model with the query points\n",
    "\n",
    "    x_querried = np.concatenate((x_querried, x_new))\n",
    "    y_querried = np.concatenate((y_querried, y_new))\n",
    "\n",
    "    # move this line out of the loop when you are increasing the iterations -> That way you won't be flooded with plots\n",
    "    plotting_method(committee, x, x_new=x_new, x_querried=x_querried, y_querried=y_querried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning on the 2AFC experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are going to replace the ground truth with our 2AFC experiment.\n",
    "\n",
    "You can basically repeat all the previous steps from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.synthetic import generate_dataset, experimental_unit, normal_ground_truth, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset parameters\n",
    "\n",
    "# number of models in the ensemble\n",
    "n_ensemble = 8\n",
    "\n",
    "# number of experimental units\n",
    "n_units = 100\n",
    "\n",
    "# number of observations per experimental unit\n",
    "n_conditions = 10\n",
    "\n",
    "# number of repetitions per condition\n",
    "n_repetitions = 1\n",
    "\n",
    "# amount of noise which we are going to add to the data\n",
    "noise_level = 0.\n",
    "\n",
    "# relative amount of train vs test samples\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've finished the introductory tutorial to active learning and the key components of modAL!\n",
    "\n",
    "Let's explore the key features of modAL even more in the next tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
