{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to synthetic experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the concept of synthetic experiments and demonstrates how to implement one using Python. \n",
    "\n",
    "Synthetic experiments are a powerful tool for studying the relationship between independent variables (factors) and dependent variables (observations) in an controlled environment. They allow us to generate data that resembles real-world data but is not as complex as data collected in the real world while being also free from the limitations, costs and biases that can arise from collecting data in the real world. \n",
    "\n",
    "This way, the later introduced concepts and algorithms from the 'Optimizing Experimental Design' course can be implemented, tested and validated easily. \n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Define a ground truth function that represents the true relationship between the independent and dependent variables.\n",
    "- Generate noisy data by adding noise to the true function values.\n",
    "- Define treatments and experimental units which will serve as the base for collecting data\n",
    "- Generating a dataset by defining a sample size, different treatments and eventually collecting samples.\n",
    "- Run a model recovery to fit a model to the collected data and recover the parameters of the ground truth function.\n",
    "- Get a feeling for the recovered model quality in dependence of the sample size and noise level\n",
    "\n",
    "You will also explore how to experiment with different noise levels, sample sizes, and conditions to see how they affect the results of the model recovery.\n",
    "\n",
    "By the end of this tutorial, you will have a strong understanding of how to implement and analyze synthetic experiments using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment description\n",
    "\n",
    "We want to examine the response times of participants in a two-alternative-forced-choice (2AFC) experiment. This experiment consists of an image which is shown for a short period of time. The image is a grid of either orange or blue tiles as shown in the picture below. \n",
    "\n",
    "We can control the experiment via the two factors **ratio** and **scatterdness**. \n",
    "**Ratio** determines the amount of blue vs orange tiles, where 0 means that the participant sees only orange tiles while 1 means that the amount of blue vs orange tiles is perfectly balanced. \n",
    "**Scatterdeness** determines how noisy the image appears, where 0 means that all orange tiles are placed on the left half while 1 means that the tiles are placed completely randomly.\n",
    "\n",
    "![static/img/2afc_grid.png](static/img/2afc_grid.png)\n",
    "\n",
    "(Image source: Trueblood J. S. et al (2021)., Urgency, Leakage, and the Relative Nature of Information Processing in\n",
    "Decision-Making.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the relevant libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add the path of the project folder to the python variables\n",
    "# That way python finds custom packages defined in the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement a ground truth which will serve us later to generate observations and thus let's us perform our experiment.\n",
    "\n",
    "In this section, you will learn to:\n",
    "- Implement the ground truth method that models the relationship between the independent variables (factors) and the dependent variable (observation).\n",
    "- Implement a method that generates noise and add it to the output of the ground truth\n",
    "- Implement a class, whose instances will resemble the experimental units (e.g. participants)\n",
    "- Create experimental units given sets of parameters and generate first observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a ground truth\n",
    "\n",
    "The ground truth can be any mathematical expression that takes in the independent variables as arguments and returns the dependent variable.\n",
    "\n",
    "In our case, we will model a 2-factor ground truth with the two factors 'x' and 'y' and their respective parameters.\n",
    "\n",
    "Replace the variable 'dependent_variable' which is currently 'None' with the actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth(x, y, parameters=np.ones(2,)):\n",
    "    \"\"\"This ground truth takes in two factors and a set of parameters and returns a response\n",
    "\n",
    "    Args:\n",
    "        x (float): The level of the first factor\n",
    "        y (float): the level of the second factor\n",
    "        parameters (Iterable[float], optional): The parameters give an individual configuration for each experimental unit. Defaults to np.ones(2,).\n",
    "\n",
    "    Returns:\n",
    "        float: Dependent variable which serves as the response\n",
    "    \"\"\"\n",
    "    \n",
    "    # this is an example of a ground truth function with two linear terms and a constant term \n",
    "    # dependent_variable = parameters[0] * x + parameters[1] * y + parameters[0] + parameters[1]\n",
    "    \n",
    "    # this is an example of a bell-shaped function which can saturate\n",
    "    dependent_variable = (1-np.exp(-np.power(x, 2)/parameters[0])) + np.power(y, parameters[1])\n",
    "    \n",
    "    return dependent_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your ground truth by giving it random values and check if it has the desired properties.\n",
    "\n",
    "Replace now the None-values with your own factor levels and parameters.\n",
    "\n",
    "This code takes in the specified levels for the factors (conditions) ratio and scatteredness, and the experimental unit (participant) parameters. The output are reponse time observation which we can visualize in a 3D-plot over the given factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the factor levels\n",
    "# you can use numpy arrays, lists or tuples\n",
    "x = np.linspace(0, 1)\n",
    "y = np.linspace(0, 1)\n",
    "parameters = [.2 , 2]\n",
    "\n",
    "# DO NOT TOUCH THE CODE FROM HERE!\n",
    "\n",
    "# set a sample size\n",
    "sample_size = len(x)\n",
    "\n",
    "# initiate the z array\n",
    "z = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "x, y = np.meshgrid(x, y)\n",
    "for i in range(sample_size):\n",
    "    z[i, :] = ground_truth(x[i], y[i], parameters)\n",
    "\n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x, y, z, cmap=cm.Blues)\n",
    "ax.set_title('Ground truth: Response times over conditions')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look good? \n",
    "\n",
    "Try some parameter configurations to get a feeling for how the response times can change from participant to participant. \n",
    "\n",
    "Afterwards you can move to the next part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a noise signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise signal is a value drawn from a normal distribution. \n",
    "\n",
    "It will be multiplied by a specified noise level and later be added to the dependent variable to get the final observation.\n",
    "\n",
    "The scale of normal distribution should be $0.1$\n",
    "\n",
    "This way we get the inherent noise, which basically any experiment comes with.\n",
    "\n",
    "Tip: numpy comes already with a method that draws values from a random distribution. Use that one! Call it with np.random.normal(mean, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(noise_level):\n",
    "\n",
    "    # add your code here:\n",
    "    \n",
    "    # draw a value from a normal distribution with the noise level defining the variance\n",
    "    noise = np.random.normal(0, noise_level)\n",
    "    \n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the noise looks as we expect it to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 1\n",
    "\n",
    "noise_sample_size = 100\n",
    "noise_samples = np.zeros(noise_sample_size)\n",
    "\n",
    "for i in range(noise_sample_size):\n",
    "    noise_samples[i] = noise(noise_level=1)\n",
    "    \n",
    "plt.plot(noise_samples, '.')\n",
    "plt.title('Collected noise samples')\n",
    "plt.ylabel('Noise')\n",
    "plt.xlabel('Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some noise to the observations and compare the previous observations without noise with the current noisy ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set noise level\n",
    "noise_level = .3\n",
    "\n",
    "# initiate the noisy z array\n",
    "z_noisy = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    z_noisy[i, :] = ground_truth(x[i], y[i], parameters)\n",
    "    \n",
    "# add noise\n",
    "for i in range(z_noisy.shape[0]):\n",
    "    for j in range(z_noisy.shape[1]):\n",
    "        z_noisy[i, j] += noise(noise_level)\n",
    "\n",
    "# make sure that each sample is bigger than 0 -> no negative response times\n",
    "z_noisy = np.max(np.stack((np.zeros_like(z_noisy), z_noisy), axis=1), axis=1)\n",
    "\n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x, y, z, cmap=cm.Blues)\n",
    "ax.set_title('Unnoisy response times over conditions')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()\n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x, y, z_noisy, cmap=cm.Blues)\n",
    "ax.set_title('Noisy response times over conditions')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()\n",
    "\n",
    "# make a plot of the difference between the unnoisy and the noisy signal\n",
    "z_error = np.abs(z - z_noisy)\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x, y, z_error, cmap=cm.Reds)\n",
    "ax.set_title('Error between unnoisy and noisy response times over factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did your observations changed? \n",
    "\n",
    "Or did they change at all? \n",
    "\n",
    "Try to increase the noise bit by bit to see the changes taking places until the signal-to-noise ratio becomes too small to see any pattern in the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an experimental unit class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will implement a class for experimental units.\n",
    "\n",
    "In the psychological context, experimental units refer to individual participants or groups of participants where each group gets one specific treatment.\n",
    "\n",
    "To account for such differences between participants, we will now implement a class which takes in:\n",
    "- a problem_solver method which refers to the ground_truth method which you already implemented.\n",
    "- a noise method which you already implemented as well.\n",
    "- a set of parameters, to account for the individual characteristics of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experimental_unit:\n",
    "    \n",
    "    def __init__(self,\n",
    "        problem_solver: Callable, \n",
    "        noise: Callable, \n",
    "        parameters: Iterable, \n",
    "        noise_level: float = 1,\n",
    "        ):\n",
    "        \n",
    "        self.problem_solver_fun = problem_solver\n",
    "        self.noise_fun = noise\n",
    "        self.parameters = parameters\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    def problem_solver(self, x, y):\n",
    "        # this method returns the dependent variable based on the independent variables x and y and the parameters\n",
    "        return self.problem_solver_fun(x, y, self.parameters)\n",
    "    \n",
    "    def noise(self, noise_level=None):\n",
    "        # this method returns the noise\n",
    "        if noise_level == None:\n",
    "            noise_level = self.noise_level\n",
    "            \n",
    "        return self.noise_fun(noise_level)\n",
    "    \n",
    "    def step(self, x, y, noise=True):\n",
    "        # this method returns the observation which is the sum of the dependent variable and the noise\n",
    "        if noise:\n",
    "            obs =  self.problem_solver(x, y) + self.noise()\n",
    "        else:\n",
    "            obs = self.problem_solver(x, y)\n",
    "        \n",
    "        # make obs an array if obs is a scalar\n",
    "        if len(obs.shape) == 0:\n",
    "            obs = obs.reshape(-1)\n",
    "            \n",
    "        return np.max(np.stack((np.zeros_like(obs), obs), axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two instances (individual participants) of the the `experimental_unit` class with different parameters and compare the collected observations.\n",
    "\n",
    "In the first step we won't add any noise so we get a feeling on how parameters influence the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace None with your own treatment combinations\n",
    "parameters_participant1 = [1, 1]\n",
    "parameters_participant2 = [2, 2]\n",
    "\n",
    "# Create two experimental units with different parameters\n",
    "participant1 = experimental_unit(\n",
    "    problem_solver=ground_truth,\n",
    "    noise=noise,\n",
    "    parameters=parameters_participant1,\n",
    "    noise_level=0,\n",
    ")\n",
    "\n",
    "participant2 = experimental_unit(\n",
    "    problem_solver=ground_truth,\n",
    "    noise=noise,\n",
    "    parameters=parameters_participant2,\n",
    "    noise_level=0,\n",
    ")\n",
    "\n",
    "# define the factor levels\n",
    "x = np.linspace(0,1)\n",
    "y = np.linspace(0,1)\n",
    "x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "sample_size = len(x)\n",
    "\n",
    "# collect the observations for each participant\n",
    "# add your code here (you can take the relevant code pieces from above):\n",
    "\n",
    "# initiate the z array\n",
    "z1 = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    z1[i, :] = participant1.step(x_mesh[i], y_mesh[i])\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z1, cmap=cm.Blues)\n",
    "ax.set_title('Response times by participant 1 over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "plt.show()\n",
    "\n",
    "# plot the observations for each participant in a surface plot\n",
    "# add your code here (you can take the relevant code pieces from above):\n",
    "\n",
    "# initiate the z array\n",
    "z2 = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    z2[i, :] = participant2.step(x_mesh[i], y_mesh[i])\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z2, cmap=cm.Blues)\n",
    "ax.set_title('Response times by participant 2 over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add some noise and see whether you can still identify the that much difference between the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take all these methods and classes, which we previously implemented to create a dataset.\n",
    "\n",
    "This dataset will have unit-to-unit variation and also trial-to-trial variation thanks to the implemented `noise` and the `experimental_unit` class.\n",
    "\n",
    "In this section you will learn to:\n",
    "- Define the relevant dataset parameters (number units, number trials per unit)\n",
    "- Define arbitrary parameter sets\n",
    "- Create a collection of experimental units based on these parameter sets\n",
    "- Perform runs and collect the data in the form of the collection (factors, observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to define the parameters of our dataset.\n",
    "This contains the extrinsic parameters over which we have full control: \n",
    "- the number of units we are going to observe\n",
    "- the number of conditions each unit will be tested on\n",
    "- the number of repetitions per conditions\n",
    "\n",
    "And the intrinsic parameter for the noise level (over this parameter we usually do not have any control but which will be useful for later analyses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset parameters\n",
    "\n",
    "# number of experimental units\n",
    "n_units = 100\n",
    "\n",
    "# number of observations per experimental unit\n",
    "n_conditions = 10\n",
    "\n",
    "# number of repetitions per condition\n",
    "n_repetitions = 1\n",
    "\n",
    "# amount of noise which we are going to add to the data\n",
    "noise_level = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the unit parameters and the conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create n_units arbitrary parameter sets to define n_units experimental units\n",
    "# we will draw the parameters from a normal distribution around 1 but which is capped to the value range of (0, inf)\n",
    "parameters = np.random.normal(1, 0.5, (n_units, 2))\n",
    "parameters = np.where(parameters < 0, 0, parameters)\n",
    "\n",
    "# create n_trials conditions which will be taken for all experimental units\n",
    "# we will draw the conditions from a uniform distribution between 0 and 1\n",
    "condition = np.random.uniform(0, 1, (n_conditions, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the parameters, we can now create instances of our experimental units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of experimental units\n",
    "experimental_units = []\n",
    "\n",
    "# create the experimental units\n",
    "for i in range(n_units):\n",
    "    experimental_units.append(\n",
    "        experimental_unit(\n",
    "            problem_solver=ground_truth,\n",
    "            noise=noise,\n",
    "            parameters=parameters[i],\n",
    "            noise_level=noise_level,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the part where we are creating the actual dataset.\n",
    "\n",
    "This basically means collecting the observations for all presented the conditions to the different experimental units.\n",
    "\n",
    "This way, we will have a 4D-Corpus of the data with the dimensions (experimental units, conditions, repetitions, unit_id+n_conditions+observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array which will be the dataset\n",
    "dataset = np.zeros((n_units, n_conditions, n_repetitions, 2+condition.shape[-1]))\n",
    "\n",
    "for i in range(n_units):\n",
    "    # here we collect the observations for each experimental unit\n",
    "    for k in range(n_repetitions):\n",
    "            # here we collect the observations for each repetition for each condition for each experimental unit\n",
    "            # dataset[i, :, k] = experimental_units[i].step(conditions[:, 0], conditions[:, 1])\n",
    "            observation = experimental_units[i].step(condition[:, 0], condition[:, 1])\n",
    "            dataset[i, :, k, 0] += i\n",
    "            dataset[i, :, k, 1:1+condition.shape[-1]] = condition\n",
    "            dataset[i, :, k, -1] = observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the dataset a little bit\n",
    "\n",
    "We will compute: \n",
    "- the mean and standard deviation (std) for each experimental unit for all conditions and repitions -> 2D arrays (n_units, n_conditions)\n",
    "- the mean and std for each condition across all experimental units and repitions -> 2D arrays (n_conditions, n_conditions)\n",
    "\n",
    "We will make:\n",
    "- an errorbar plot for the unit-wise mean and std for one condition to show how the observations of one condition can differ from unit to unit \n",
    "- a 2D Hist for the condition-wise std across all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index for the condition which will be observed\n",
    "condition_index = 0\n",
    "\n",
    "# compute the mean and std for each unit and condition\n",
    "unit_mean = np.mean(dataset[:, :, :, -1], axis=2)\n",
    "unit_std = np.std(dataset[:, :, :, -1], axis=2)\n",
    "\n",
    "# compute the mean and std for each condition\n",
    "condition_mean = np.mean(dataset[:, :, :, -1], axis=2)\n",
    "condition_std = np.std(condition_mean, axis=0)\n",
    "\n",
    "# plot an errorbar plot to visualize the unit means and stds for the selected condition\n",
    "plt.errorbar(\n",
    "    np.arange(n_units),\n",
    "    unit_mean[:, condition_index],\n",
    "    yerr=unit_std[:, condition_index],\n",
    "    fmt='o',\n",
    ")\n",
    "plt.title('Unit means and stds for condition {}'.format(condition_index))\n",
    "plt.xlabel('Unit')\n",
    "plt.ylabel('Mean')\n",
    "plt.show()\n",
    "\n",
    "# visualize the condition stds across all experimental units over the conditions\n",
    "plt.bar(\n",
    "    np.arange(n_conditions),\n",
    "    condition_std,\n",
    ")\n",
    "plt.title('Condition stds across all experimental units')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Std')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you expect the std to change for an increased amount of trials and/or noise?\n",
    "\n",
    "Test your assumption by changing the value for the amount of trials and/or noise an re-run the code of this section!\n",
    "\n",
    "What did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will take the previously generated dataset to fit two different models:\n",
    "1. linear regression model from the `sklearn` package.\n",
    "2. feed-forward neural network from the `torch` package\n",
    "\n",
    "The linear regression model is quite easy to fit thanks to its limited amount of parameters but fits only on a group-level, i.e. it does not recover the individual differences between the participants. The applied neural network model, on the other hand, has a layer to processes the conditions AND the unit-id to fit individual behavior. Like that the neural network can process group-level information given by the whole dataset as well as indivual-level information given by single experimental units.\n",
    "\n",
    "In this section you will learn to:\n",
    "- Prepare data for model fitting\n",
    "- Use data to fit a model\n",
    "- Get a feeling for the significance of noise and sample size on the quality of the recovered model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "So first we need to prepare the data for the further steps.\n",
    "\n",
    "This will be done by:\n",
    "1. Creating a 2D matrix for the generated samples of the shape `(samples, features)` with features being `(unit_id, ratio, scatteredness, observation)`\n",
    "1. Shuffling along the the `samples` axis to break up any correlations other than condition-observation\n",
    "1. Splitting the data into training and test data\n",
    "    - Training data is used for fitting the model\n",
    "    - Test data is used for evaluating the model's generalization capability and making sure it did not simply memorized the samples (i.e. overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to training and test data\n",
    "data_recovery = dataset.reshape(-1, dataset.shape[-1])\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data_recovery)\n",
    "\n",
    "# Split the data into train and test data\n",
    "train_ratio = 0.8  # define the amount of data which will be used for training\n",
    "train_data = data_recovery[:int(len(data_recovery) * train_ratio)]  # take the first train_ratio*100% of the data for training\n",
    "test_data = data_recovery[int(len(data_recovery) * train_ratio):]  # take the remaining data for testing\n",
    "\n",
    "print(f'Number of training samples: {len(train_data)}')\n",
    "print(f'Number of test samples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a linear regression model\n",
    "\n",
    "Next we need to implement the model fitting algorithm.\n",
    "\n",
    "To do that, we will use a simple linear regression estimator from the sklearn package.\n",
    "\n",
    "The `LinearRegression` estimator works only on the group level which is why we don't have to provide the `unit_id` to it. It would get only confused ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(train_data[:, 1:-1], train_data[:, -1][:, None])\n",
    "\n",
    "# Recovered parameters\n",
    "recovered_a = model.coef_[0, 0]\n",
    "recovered_b = model.coef_[0, 1]\n",
    "\n",
    "# print the recovered parameters\n",
    "a_true = np.round(np.mean(parameters[:, 0]), 3)\n",
    "b_true = np.round(np.mean(parameters[:, 1]), 3)\n",
    "print('After running experiments on {} units with {} conditions and {} repetitions per condition, we recovered the following parameters:'.format(n_units, n_conditions, n_repetitions))\n",
    "print('True parameters (mean across all units): a = {}, b = {}'.format(a_true, b_true))\n",
    "print('Recovered parameters: a = {}, b = {}'.format(np.round(recovered_a, 3), np.round(recovered_b, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to test the fitted model with the test data.\n",
    "\n",
    "We will compute the prediction accuracy with a sklearn metric and plot the model predictions over the given conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# use the fitted model to predict the test observations\n",
    "observations_predicted = model.predict(test_data[:, 1:-1])\n",
    "\n",
    "# compute the means of the two datasets\n",
    "original_mean = np.mean(test_data[:, -1][:, None]) \n",
    "recovered_mean = np.mean(observations_predicted)\n",
    "\n",
    "# compute prediciton error\n",
    "prediction_accuracy = mean_squared_error(test_data[:, -1][:, None], observations_predicted)\n",
    "print('Mean squared error: {}'.format(prediction_accuracy))\n",
    "\n",
    "# compare the means\n",
    "print('Original mean: {}'.format(original_mean))\n",
    "print('Recovered mean: {}'.format(recovered_mean))\n",
    "\n",
    "# plot the recovered response times over the conditions\n",
    "# initiate the z array\n",
    "z_recovered = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    condition = np.stack((x_mesh[i], y_mesh[i]), axis=-1)\n",
    "    z_recovered[i, :] = model.predict(condition).reshape(-1)\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z_recovered, cmap=cm.Reds)\n",
    "ax.set_title('Recovered response times by a linear model over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result on the test data look like?\n",
    "\n",
    "A small error - compared to the average amplitude of the observation - is acceptable since it's difficult to get a perfect match. For most cases it's even impossible.\n",
    "\n",
    "Get a feeling for the significance of noise and sample size on the quality of a fitted model and its predictive power by adjusting the number of units, conditions, repetitions and also the noise level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a neural network\n",
    "\n",
    "Now we are going to train the feed-forward neural network (FFN) with the same data as the linear regression model. Unlike the linear model, you have to specify the training epochs by hand. More advanced training algorithms would specify convergence criteria, but that's not really necessary in this case due to the simplicity of data/model and training speed.\n",
    "\n",
    "We can further wrap the FFN with the `FFNRegressor` class which inherits from `sklearn.base.BaseEstimator` to (1) make it compatible with any sklearn-based package and (2) get a pre-implemented fitting procedure which is sufficient for most cases including like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.regressors import FFN, FFNRegressor\n",
    "\n",
    "# train a neural network model\n",
    "# train_data = tensor(train_data)\n",
    "model_ffn = FFNRegressor(FFN(n_units, 2), max_epochs=100, lr=0.1)\n",
    "model_ffn.fit(train_data[:, 0:-1], train_data[:, -1][:, None].astype(np.float32))\n",
    "\n",
    "# get predictions on the test data\n",
    "predictions = model_ffn.predict(test_data[:, 0:-1])\n",
    "loss = mean_squared_error(test_data[:, -1][:, None], predictions)\n",
    "print(f\"Test loss: {loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the train and test error look like compared to the linear model? Theoretically, the network should be much better in approximating non-linear functions like our response time function. What could you change in order to make the FFN more predictive? \n",
    "\n",
    "You can also inspect the approximated behavior for individual experimental units in the next cell by plotting the response times. Maybe this gives you a better idea of the best possible adjustments.\n",
    "\n",
    "Try to find the most data-efficient setup with respect to number of participants, noise, number of recorded conditions and repitions.\n",
    "\n",
    "By setting the `unit_id` variable, you can plot different participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the recovered response times over the conditions for single participants\n",
    "unit_id = 6\n",
    "\n",
    "# initiate the z array\n",
    "z_recovered = np.zeros((sample_size, sample_size))\n",
    "z_real = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    condition = np.stack((x_mesh[i], y_mesh[i]), axis=-1)\n",
    "    unit_id_array=np.full((condition.shape[0], 1), unit_id)\n",
    "    X = np.concatenate((unit_id_array, condition), axis=-1)\n",
    "    \n",
    "    z_recovered[i, :] = model_ffn.predict(X).reshape(-1)\n",
    "    z_real[i, :] = experimental_units[unit_id].step(x_mesh[i], y_mesh[i], noise=False)\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z_recovered, cmap=cm.Reds, alpha=0.5)\n",
    "ax.plot_surface(x_mesh, y_mesh, z_real, cmap=cm.Blues, alpha=0.5)\n",
    "ax.set_title('Real (blue) vs recovered (red) response times by a neural network over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You finished the first tutorial.\n",
    "\n",
    "Now you should have a good idea about:\n",
    "- How to implement a synthetic experiment\n",
    "- How to get unit-to-unit and trial-to-trial variation into your dataset\n",
    "- How the sample size and noise affects the recovery of parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
