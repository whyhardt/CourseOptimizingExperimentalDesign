{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Sampling Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will learn about different random sampling strategies and in which cases they are useful.\n",
    "\n",
    "This notebook covers the following strategies:\n",
    "- sampling from a normal distribution\n",
    "- sampling from a uniform distribution\n",
    "- sampling with a latin hypercube\n",
    "\n",
    "You will explore the generated treatment combinations and compare them visually.\n",
    "\n",
    "You are also going to scale the gathered treatments to a certain value range to keep the differently collected treatments as comparable as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we set the general parameters and methods which will be used at each stage of this notebook.\n",
    "\n",
    "The parameters in this case are:\n",
    "- the sample size\n",
    "- the number of factors.\n",
    "- the value range for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "n_factors = 2\n",
    "condition_range = (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Before gathering the different treatments, we will implement a scaling method.\n",
    "\n",
    "This scaling method will scale the gathered treatments to a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, range: Iterable):\n",
    "    \"\"\"\n",
    "    Scale the input to the given range.\n",
    "    Args:\n",
    "        x: the input to scale. Can be any type that can be converted to a numpy array.\n",
    "        range: the range to scale to. Must be a tuple of length 2, with the first element being the minimum and the second element being the maximum.\n",
    "    \"\"\"\n",
    "    assert len(range) == 2, \"range must be an iterable of length 2.\"\n",
    "    \n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    x = x * (range[1] - range[0]) + range[0]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work properly? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "array = scale(array, condition_range)\n",
    "\n",
    "print(f\"The scaled array is {array}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "\n",
    "The first sampling strategy which we will implement is sampling from a normal distribution.\n",
    "\n",
    "We take the numpy.random.normal method.\n",
    "\n",
    "You can take a look at the [official documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html), if you are not familiar with it.\n",
    "\n",
    "Use the parameters from the Setup section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the treatment combinations with a normal distribution\n",
    "# add your code here\n",
    "treatments_normal = np.random.normal(size=(n_samples, n_factors))\n",
    "\n",
    "# scale the treatment combinations to the range (-1, 1) with your scale function\n",
    "# add your code here\n",
    "treatments_normal = scale(treatments_normal, condition_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(treatments_normal[:, 0], treatments_normal[:, 1])\n",
    "plt.title('Random sampling from a normal distribution')\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered treatmens change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a uniform distribution\n",
    "\n",
    "The second sampling strategy which we will implement is sampling from a uniform distribution.\n",
    "\n",
    "We take the numpy.random.normal method.\n",
    "\n",
    "You can take a look at the [official documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html), if you are not familiar with it.\n",
    "\n",
    "Use the parameters from the Setup section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments_uniform = None\n",
    "\n",
    "# generate the treatment combinations with a uniform distribution\n",
    "# add your code here\n",
    "treatments_uniform = np.random.uniform(low=0, high=1, size=(n_samples, n_factors))\n",
    "\n",
    "# We do not have to scale the uniform distribution in this case because it is already in the range (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(treatments_uniform[:, 0], treatments_uniform[:, 1])\n",
    "plt.title('Random sampling from a uniform distribution')\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered treatmens change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Latin Hypercube\n",
    "\n",
    "Lastly, we will implement the latin hypercube strategy.\n",
    "\n",
    "For that we will take the scipy.stats.qmc.LatinHypercube method.\n",
    "\n",
    "Please make yourself familiar with it through the [official documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html).\n",
    "\n",
    "And here again - use the parameters from the setup.\n",
    "\n",
    "Tip: We import the LatinHypercube class from scipy. Use the 'random' method of this class to create the treatment combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.qmc import LatinHypercube\n",
    "\n",
    "# generate the treatment combinations with a normal distribution\n",
    "# add your code here\n",
    "latinhypercube = LatinHypercube(d=n_factors)\n",
    "treatments_lhc = latinhypercube.random(n_samples)\n",
    "\n",
    "# scale the treatment combinations to the range (-1, 1) with your scale function\n",
    "# add your code here\n",
    "treatments_lhc = scale(treatments_lhc, condition_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(treatments_lhc[:, 0], treatments_lhc[:, 1])\n",
    "plt.title('Random sampling from a Latin Hypercube')\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Does it look any different from the normal distribution?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered treatmens change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed analysis of the obtained results\n",
    "\n",
    "Here we will now analyze the obtained results with respect to their coverage.\n",
    "\n",
    "We will compute the coverage and plot a 2D histogram of the differently gathered treatments. \n",
    "\n",
    "This will give us a better idea of the differences - especially between uniform and Latin Hypercube sampling - than simple scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, copy the sampling commands which you already implemented into the following cell so we do not have to re-run each cell when changing the sample size.\n",
    "\n",
    "(And don't forget to scale here as well)\n",
    "\n",
    "For the Latin Hypercube:\n",
    "    Make this time use of the 'optimization' keyword and watch the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "np.random.seed(0)\n",
    "\n",
    "treatments_normal = scale(np.random.normal(0, 1, (n_samples, n_factors)), condition_range)\n",
    "treatments_uniform = np.random.uniform(condition_range[0], condition_range[1], (n_samples, n_factors))\n",
    "treatments_lhc = scale(LatinHypercube(d=n_factors, optimization='lloyd').random(n_samples), condition_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the coverage of the treatments with respect to the design space!\n",
    "\n",
    "The steps for our analysis look as follows:\n",
    "1. Compute the target coverage which is $target=\\frac{n_\\text{samples}}{n_\\text{bins}**n_\\text{factors}}$\n",
    "2. Compute 2D Histograms for each of the methods where the number of samples per bin is counted\n",
    "3. Compute the bin-wide coverage for each method\n",
    "4. Compute under-, over- and highly over-represented bins ($bin_\\text{count}<target$; $bin_\\text{count}>target$; $bin_\\text{count}>target*threshold$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins = 10\n",
    "threshold_highly_overrepresented = 2\n",
    "\n",
    "# compute the target coverage\n",
    "target_coverage = np.max((n_samples/(hist_bins**n_factors), 1))\n",
    "threshold_highly_overrepresented = target_coverage * threshold_highly_overrepresented\n",
    "print(f'Target coverage: {target_coverage} samples per bin')\n",
    "\n",
    "# compute 2D histograms for each sampling method with the numpy method\n",
    "hist_normal = np.histogram2d(treatments_normal[:, 0], treatments_normal[:, 1], bins=hist_bins)[0]\n",
    "hist_uniform = np.histogram2d(treatments_uniform[:, 0], treatments_uniform[:, 1], bins=hist_bins)[0]\n",
    "hist_lhc = np.histogram2d(treatments_lhc[:, 0], treatments_lhc[:, 1], bins=hist_bins)[0]\n",
    "\n",
    "# compute the actual coverage for each sampling method\n",
    "coverage_normal = np.round(np.sum(hist_normal > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "coverage_uniform = np.round(np.sum(hist_uniform > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "coverage_lhc = np.round(np.sum(hist_lhc > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of underrepresented bins for each sampling method\n",
    "underrep_normal = round(np.sum(hist_normal < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "underrep_uniform = round(np.sum(hist_uniform < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "underrep_lhc = round(np.sum(hist_lhc < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of overrepresented bins for each sampling method\n",
    "overrep_normal = round(np.sum(hist_normal > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "overrep_uniform = round(np.sum(hist_uniform > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "overrep_lhc = round(np.sum(hist_lhc > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of highly overrepresented bins for each sampling method\n",
    "highly_overrep_normal = round(np.sum(hist_normal > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "highly_overrep_uniform = round(np.sum(hist_uniform > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "highly_overrep_lhc = round(np.sum(hist_lhc > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# print the results in a table\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n",
    "print(f'{\"Sampling method\":<25} {\"Coverage [%]\":<15} {\"Underrepresented [%]\":<25} {\"Overrepresented [%]\":<25} {\"Highly overrepresented [%]\":<25}')\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n",
    "print(f'{\"Normal distribution\":<25} {coverage_normal:<15} {underrep_normal:<25} {overrep_normal:<25} {highly_overrep_normal:<25}')\n",
    "print(f'{\"Uniform distribution\":<25} {coverage_uniform:<15} {underrep_uniform:<25} {overrep_uniform:<25} {highly_overrep_uniform:<25}')\n",
    "print(f'{\"Latin Hypercube\":<25} {coverage_lhc:<15} {underrep_lhc:<25} {overrep_lhc:<25} {highly_overrep_lhc:<25}')\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the corresponding plots!\n",
    "\n",
    "The plotting is already implemented to make your life a little bit easier. \n",
    "\n",
    "You are still very welcome to go through the plotting code on your own as oftentimes this can be a quite challenging part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = True\n",
    "\n",
    "# make a figure with three subplots for the three sampling methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# plot the samples from the normal distribution on the first subplot\n",
    "axes[0].hist2d(treatments_normal[:, 0], treatments_normal[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[0].scatter(treatments_normal[:, 0], treatments_normal[:, 1], color='orange')\n",
    "axes[0].set_title('normal distribution')\n",
    "axes[0].set_xlabel('Factor 1')\n",
    "axes[0].set_ylabel('Factor 2')\n",
    "\n",
    "# plot the samples from the uniform distribution on the second subplot\n",
    "axes[1].hist2d(treatments_uniform[:, 0], treatments_uniform[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[1].scatter(treatments_uniform[:, 0], treatments_uniform[:, 1], color='orange')\n",
    "axes[1].set_title('uniform distribution')\n",
    "axes[1].set_xlabel('Factor 1')\n",
    "\n",
    "# plot the LHS samples on the third subplot\n",
    "axes[2].hist2d(treatments_lhc[:, 0], treatments_lhc[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[2].scatter(treatments_lhc[:, 0], treatments_lhc[:, 1], color='orange')\n",
    "axes[2].set_title('Latin Hypercube')\n",
    "axes[2].set_xlabel('Factor 1')\n",
    "\n",
    "# set figure title\n",
    "fig.suptitle('Sampling from')\n",
    "\n",
    "# share y axis\n",
    "axes[1].sharey(axes[0])\n",
    "axes[2].sharey(axes[0])\n",
    "# hide y axis for the second and third subplot\n",
    "axes[1].get_yaxis().set_visible(False)\n",
    "axes[2].get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing all these analysis: What's your impression of the different random sampling methods?\n",
    "\n",
    "Can you think of use-cases for each of the method?\n",
    "\n",
    "Can you tell now a difference between the uniform and Latin Hypercube sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's set up our own experiment with these sampling methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we get significantly different results for the presented random sampling methods with respect to noise, dataset size and dataset structure (i.e. number of repetitions per condition vs number of conditions).\n",
    "First of all, we have to import the existing classes for the experimental unit, the dataset generator, and the feed-forward-network regressor. You should be familiar with these from the last tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library, package and class imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)\n",
    "\n",
    "from resources.synthetic import experimental_unit, normal_ground_truth, noise, generate_dataset\n",
    "from resources.regressors import FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set up the sampling strategy and generate the dataset. You can use the respective parts from the first tutorial. Set the number of participants, tested conditions, repitions and the noise level as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset parameters\n",
    "\n",
    "# number of experimental units\n",
    "n_units = 100\n",
    "\n",
    "# number of observations per experimental unit\n",
    "n_conditions = 1000\n",
    "\n",
    "# number of repetitions per condition\n",
    "n_repetitions = 1\n",
    "\n",
    "# amount of noise which we are going to add to the data\n",
    "noise_level = 0.\n",
    "\n",
    "# relative amount of train vs test samples\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_units = None\n",
    "train_data = None\n",
    "test_data = None\n",
    "parameters = None\n",
    "conditions = None\n",
    "\n",
    "# add your code here; don't forget that the final class of your dataset should be torch.Tensor\n",
    "# you can use the generate_dataset method to create a dataset which you can then split into train and test samples\n",
    "# e.g.:\n",
    "# dataset, dataset_flat = generate_dataset(experimental_units, conditions, n_repetitions, shuffle=True)\n",
    "# train_data = dataset_flat[:int(len(dataset_flat)*train_ratio)]\n",
    "# test_data = dataset_flat[int(len(dataset_flat)*train_ratio):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the regressor based on the generated dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetRegressor(FFN(n_units, 2), max_epochs=100)\n",
    "model.fit(train_data[:, 0:-1], train_data[:, -1][:, None])\n",
    "\n",
    "prediction = model.predict(test_data[:, 0:-1])\n",
    "loss = mean_squared_error(test_data[:, -1][:, None].numpy(), prediction)\n",
    "\n",
    "print(f\"Test loss: {loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the trained model in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the recovered response times over the conditions\n",
    "# define the factor levels\n",
    "x = np.linspace(0, 1)\n",
    "y = np.linspace(0, 1)\n",
    "x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "sample_size = len(x)\n",
    "\n",
    "# initiate the z array\n",
    "z_recovered = np.zeros((sample_size, sample_size))\n",
    "z_real = np.zeros((sample_size, sample_size))\n",
    "\n",
    "unit_id = 30\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    condition = torch.tensor(np.stack((x_mesh[i], y_mesh[i]), axis=-1))\n",
    "    unit_id_array = torch.full((condition.shape[0], 1), unit_id)\n",
    "    X = torch.cat((unit_id_array, condition), axis=-1)\n",
    "    \n",
    "    z_recovered[i, :] = model.predict(X).reshape(-1)\n",
    "    z_real[i, :] = experimental_units[unit_id].step((x_mesh[i], y_mesh[i]), noise=False)\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z_recovered, cmap=cm.Reds, alpha=0.5)\n",
    "ax.plot_surface(x_mesh, y_mesh, z_real, cmap=cm.Blues, alpha=0.5)\n",
    "ax.set_title('Real (blue) vs recovered (red) response times by a neural network over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've finished the tutorial on random sampling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
