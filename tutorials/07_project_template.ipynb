{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the project template for implementing your own experimentalist and test it against a random experimentalist in a closed-loop.\n",
    "\n",
    "Our closed-loop will consist of three parts:\n",
    "1. The experiment runner which executes the 2AFC experiment\n",
    "2. The theorist which is fitted on the conditions and collected observations from the experiment\n",
    "3. The experimentalist which is taking in any useful information from the sampled conditions, observations, theorist predictions and uncertainty to give us the next best possible condition\n",
    "\n",
    "The 2AFC experiment will include a baseline noise, indivual differences for each participant and an additional non-uniform noise distribution which is dependent on the given conditions.\n",
    "The non-uniform noise distribution will be a simple linear function $f_{noise}(ratio, scatteredness)$ augmenting our baseline noise $noise_{baseline}$ according to $noise = noise_{baseline} + f(ratio, scatteredness)$.\n",
    "This is a plausible assumption given that the parameters ratio and scatteredness are used to generate a random image with blue and orange tiles. Some random images yield faster response times while other images may yield slower response times. This assumption about non-uniform noise adds an interesting and realistic flavour to our experiment making some conditions worth exploring more (high ratio and high scatteredness) and others less (low ratio and low scatteredness).\n",
    "\n",
    "The theorist will be the ensemble neural network regressor which you know from the model disagreement sampling. Since neural networks don't come automatically with a uncertainty estimation but many experimentalists need some sort of uncertainty measure, we have to approximate this uncertainty through the model disagreement.\n",
    "\n",
    "The experimentalist will be your very own implementation of the presented algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from typing import List\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from autora.experimentalist.random import random_pool\n",
    "from autora.variable import Variable, VariableCollection\n",
    "\n",
    "from resources.synthetic import twoafc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the synthetic experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic experiment parameters\n",
    "n_participants = 10  # Number of synthetic participants\n",
    "noise_baseline = 0.  # noise added to the experiment runners observations\n",
    "individual_difference_level = 2  # the level of individual differences defines the amount of variation in the participant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parameters for each synthetic participant\n",
    "parameters = np.random.normal(1, 1, (n_participants, 2))\n",
    "parameters = (parameters - parameters.min()) / (parameters.max() - parameters.min()) + 1e-6\n",
    "parameters = parameters * individual_difference_level\n",
    "\n",
    "# Create the experiment\n",
    "experiment = twoafc(parameters, noise_level=noise_baseline)\n",
    "\n",
    "print(f\"Created experiment with {n_participants} synthetic participants\")\n",
    "print(f\"Noise level: {noise_baseline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-uniform noise\n",
    "\n",
    "The non-uniform noise returns noise values which are dependent on the ratio and scatteredness. You can simply add this non-uniform noise onto your collected observations from the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_uniform_noise(ratio, scatteredness, max_noise: float = 0.3) -> float:\n",
    "    if isinstance(ratio, float):\n",
    "        ratio = np.array(ratio).reshape(1, 1)\n",
    "        scatteredness = np.array(scatteredness).reshape(1, 1)\n",
    "        \n",
    "    noise_level = (ratio + scatteredness) / 2 * max_noise\n",
    "    \n",
    "    non_uniform_noise = np.array([np.random.randn(0, noise_level) for _ in range(ratio.shape[0])])\n",
    "    return non_uniform_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the theorist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use two instances of our theorist.\n",
    "\n",
    "One is fitted on the samples gathered by the random sampling experimentalist and the second is fitted on samples gathered by our own experimentalist.\n",
    "\n",
    "That way we can compare the effectiveness of the experimentalists against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.regressors import FFN, FFNRegressor\n",
    "\n",
    "def setup_theorist(n_models: int, n_participants: int) -> List[BaseEstimator]:\n",
    "\n",
    "    theorist = []\n",
    "    for _ in range(n_models):\n",
    "        theorist.append(\n",
    "            FFNRegressor(\n",
    "                FFN(n_units=n_participants, n_conditions=2),\n",
    "                max_epochs=100,\n",
    "                lr=0.1,\n",
    "                verbose=False\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return theorist\n",
    "\n",
    "n_models = 10\n",
    "theorist_random = setup_theorist(n_models, n_participants)\n",
    "theorist_oed = setup_theorist(n_models, n_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the experimentalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oed_experimentalist(experiment, current_conditions: pd.DataFrame, ensemble: List[BaseEstimator], num_samples=1, pool_size=10000) -> pd.DataFrame:\n",
    "    \n",
    "    new_conditions = None\n",
    "    \n",
    "    # add your own code here\n",
    "    \n",
    "    # Return as DataFrame\n",
    "    ivs = experiment.variables.independent_variables\n",
    "    column_names = [iv.name for iv in ivs]\n",
    "    new_conditions = pd.DataFrame(new_conditions, columns=column_names)\n",
    "    \n",
    "    return new_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our approaches we are going to (1) fit the theorists on the conditions and observations given by the two experimentalists and (2) collect predictions on a big set of test data.\n",
    "\n",
    "The test data will be based on 1000 random conditions collected from our experiment runner without any noise. This is an assumption which holds only in a synthetic scenario because in a real experiment we (1) may wouldn't be able to collect such a big dataset for testing and (2) couldn't get the observations without noise. \n",
    "\n",
    "But in the synthetic scenario this assumption helps us to verify the effectiveness of our experimentalist easily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.random import random_pool\n",
    "\n",
    "# collect the random test conditions\n",
    "test_conditions = random_pool(experiment.variables, num_samples=1000, random_state=42)\n",
    "\n",
    "# collect the observations without any noise\n",
    "test_observations = experiment.run(test_conditions, noise_level_run=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the closed-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up the closed-loop and run it.\n",
    "\n",
    "Please keep in mind that most experimentalists need an initial set of conditions which can be collected e.g. randomly.\n",
    "Perhaps your algorithm takes already care of that but if not you have to collect them manually.\n",
    "\n",
    "Keep also in mind to sample as many new conditions with the random experimentalist as you are sampling with your own experimentalist. (recommendation: one sample per cycle)\n",
    "\n",
    "Further, you can keep track of the sampled conditions by plotting them cycle by cycle alongside the old conditions to see whether your experimentalist gets stuck in some regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cycles = 20\n",
    "n_new_samples = 1\n",
    "max_non_uniform_noise = 0.3\n",
    "\n",
    "# adjust as needed\n",
    "conditions_oed = None\n",
    "conditions_random = None\n",
    "\n",
    "# collect the mean squared error between the test observations and the test predictions\n",
    "mse_oed_all = np.zeros(n_cycles)\n",
    "mse_random_all = np.zeros(n_cycles)\n",
    "\n",
    "for cycle in range(n_cycles):\n",
    "    \n",
    "    # get new conditions\n",
    "    new_conditions_oed = oed_experimentalist()  # add here the inputs as needed\n",
    "    conditions_oed = np.concatenate((conditions_oed, new_conditions_oed))\n",
    "    \n",
    "    new_conditions_random = random_pool(experiment.variables, num_samples=n_new_samples)\n",
    "    conditions_random = np.concatenate((conditions_random, new_conditions_random))\n",
    "    \n",
    "    # collect data from experiment\n",
    "    experiment_data_oed = experiment.run(conditions_oed).to_numpy()\n",
    "    experiment_data_random = experiment.run(conditions_random).to_numpy()\n",
    "    \n",
    "    # add non-uniform noise on top\n",
    "    experiment_data_oed[:, -1] += non_uniform_noise(conditions_oed.values[:, 0], conditions_oed.values[:, 1], max_noise=max_non_uniform_noise)\n",
    "    experiment_data_random[:, -1] += non_uniform_noise(conditions_random.values[:, 0], conditions_random.values[:, 1], max_noise=max_non_uniform_noise)\n",
    "    \n",
    "    # fit your model\n",
    "    for index_model, model in enumerate(theorist_oed):\n",
    "        theorist_oed[index_model].fit(experiment_data_oed[:, :-1], experiment_data_oed[:, -1:])\n",
    "        theorist_random[index_model].fit(experiment_data_random[:, :-1], experiment_data_random[:, -1:])\n",
    "    \n",
    "    # get test predictions\n",
    "    mse_oed = 0\n",
    "    mse_random = 0\n",
    "    for index_model, model in enumerate(theorist_oed):\n",
    "        pred_oed = theorist_oed[0].predict(test_conditions)\n",
    "        mse_oed += np.mean((test_observations - pred_oed)**2) / len(theorist_oed)\n",
    "        \n",
    "        pred_random = theorist_random[0].predict(test_data[:, :-1])\n",
    "        mse_random += np.mean((test_data[:, -1:]-pred_random)**2) / len(theorist_random)\n",
    "        \n",
    "    mse_oed_all[cycle] += mse_oed\n",
    "    mse_random_all[cycle] += mse_random\n",
    "    \n",
    "    # print results\n",
    "    print(f\"Cycle {cycle+1}/{n_cycles}: Disagreement MSE = {np.round(mse_oed, 8)}; Random MSE = {np.round(mse_random, 8)}\")\n",
    "    \n",
    "plt.plot(mse_random_all, label='random')\n",
    "plt.plot(mse_oed_all, label='disagreement')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your analysis should include how the final MSE looks like when using the random experimentalist and when using your own experimentalist.\n",
    "\n",
    "In order to account for random effects you should run the entire closed-loop several times to be able to compute averaged MSEs over cycles as well as their standard deviation. \n",
    "Plot both within one plot.\n",
    "\n",
    "Then you can do a qualitative analysis of how well single participants are matched by a single ensemble member of the theorist.\n",
    "\n",
    "Do your analysis in by altering the following two factors: \n",
    "1. max_non_uniform_noise \n",
    "2. individual_difference_level\n",
    "\n",
    "Both factors change the information content of specific areas of our experimental design space and thus make these areas more interesting to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_id = 7\n",
    "\n",
    "experiment.plotter(\n",
    "    participant_id=participant_id,\n",
    "    model=theorist_oed[0]\n",
    ")\n",
    "\n",
    "experiment.plotter(\n",
    "    participant_id=participant_id,\n",
    "    model=theorist_random[0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
