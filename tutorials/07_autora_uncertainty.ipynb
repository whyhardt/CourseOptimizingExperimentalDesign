{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Uncertainty-Based Active Learning with AutoRA\n",
        "\n",
        "Welcome to uncertainty-based active learning! In this tutorial, you'll learn how to use model uncertainty to intelligently select which experiments to run next.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "- Understand how models can quantify prediction uncertainty\n",
        "- Use Gaussian Process models that provide natural uncertainty estimates\n",
        "- Implement uncertainty sampling with AutoRA's uncertainty experimentalist\n",
        "- Compare random vs. uncertainty-based sampling strategies\n",
        "- Analyze information gain and uncertainty reduction over time\n",
        "\n",
        "## Why Uncertainty Sampling?\n",
        "\n",
        "**Key Insight**: Not all samples are equally informative!\n",
        "\n",
        "Consider two regions in your design space:\n",
        "- **Region A**: Model predicts confidently (low uncertainty)\n",
        "- **Region B**: Model is very uncertain (high uncertainty)\n",
        "\n",
        "**Question**: Which region should we sample next?\n",
        "\n",
        "**Answer**: Region B! We learn more from samples where we're uncertain.\n",
        "\n",
        "This is the core principle of **uncertainty sampling**: Query the model where it's most uncertain.\n",
        "\n",
        "## Information Theory Connection\n",
        "\n",
        "Recall from the information theory tutorial:\n",
        "\n",
        "**Entropy** quantifies uncertainty:\n",
        "$$H(Y) = -\\sum_y p(y) \\log p(y)$$\n",
        "\n",
        "**Mutual Information** quantifies information gain:\n",
        "$$I(X; Y) = H(Y) - H(Y|X)$$\n",
        "\n",
        "Uncertainty sampling aims to maximize mutual information between our observations and the underlying function!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Add project folder to path\n",
        "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if target_folder not in sys.path:\n",
        "    sys.path.append(target_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## Part 1: Recap - Information Theory & Uncertainty\n",
        "\n",
        "Let's quickly review key concepts from the information theory tutorial.\n",
        "\n",
        "### Entropy: Measuring Uncertainty\n",
        "\n",
        "Entropy measures the average \"surprise\" or uncertainty in a distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy(probabilities):\n",
        "    \"\"\"Compute Shannon entropy for a probability distribution\"\"\"\n",
        "    # Remove zero probabilities to avoid log(0)\n",
        "    p = np.array(probabilities)\n",
        "    p = p[p > 0]\n",
        "    return -np.sum(p * np.log2(p))\n",
        "\n",
        "# Example: Fair coin vs. biased coin\n",
        "fair_coin = [0.5, 0.5]\n",
        "biased_coin = [0.9, 0.1]\n",
        "certain = [1.0, 0.0]\n",
        "\n",
        "print(\"Entropy Examples:\")\n",
        "print(f\"  Fair coin [0.5, 0.5]: {entropy(fair_coin):.3f} bits\")\n",
        "print(f\"  Biased coin [0.9, 0.1]: {entropy(biased_coin):.3f} bits\")\n",
        "print(f\"  Certain [1.0, 0.0]: {entropy(certain):.3f} bits\")\n",
        "print(\"\\nHigher entropy = more uncertainty!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "### Mutual Information: Measuring Information Gain\n",
        "\n",
        "Mutual information quantifies how much knowing X reduces uncertainty about Y.\n",
        "\n",
        "$$I(X; Y) = H(Y) - H(Y|X)$$\n",
        "\n",
        "In active learning:\n",
        "- $Y$ = the true function we're trying to learn\n",
        "- $X$ = our observations\n",
        "- $I(X; Y)$ = information gain from observing $X$\n",
        "\n",
        "**Goal**: Select observations that maximize mutual information!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## Part 2: Gaussian Processes - Models with Built-in Uncertainty\n",
        "\n",
        "Gaussian Processes (GPs) are special because they provide both:\n",
        "1. **Predictions**: $\\mu(x)$ - mean prediction at point $x$\n",
        "2. **Uncertainty**: $\\sigma(x)$ - standard deviation at point $x$\n",
        "\n",
        "Let's see this in action with a simple 1D example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create simple 1D ground truth\n",
        "def ground_truth_1d(x):\n",
        "    return np.sin(3 * x) + 0.3 * np.cos(9 * x)\n",
        "\n",
        "# Sample a few points\n",
        "X_train_1d = np.array([[0.1], [0.4], [0.8]])\n",
        "y_train_1d = ground_truth_1d(X_train_1d.ravel())\n",
        "\n",
        "# Create GP model\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(0.1, (1e-2, 1e2)) + WhiteKernel(noise_level=0.01)\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
        "gp.fit(X_train_1d, y_train_1d)\n",
        "\n",
        "# Make predictions with uncertainty\n",
        "X_test_1d = np.linspace(0, 1, 200).reshape(-1, 1)\n",
        "y_pred, y_std = gp.predict(X_test_1d, return_std=True)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.fill_between(X_test_1d.ravel(), \n",
        "                 y_pred - 2*y_std, \n",
        "                 y_pred + 2*y_std, \n",
        "                 alpha=0.3, \n",
        "                 label='95% Confidence Interval')\n",
        "plt.plot(X_test_1d, y_pred, 'b-', linewidth=2, label='GP Mean Prediction')\n",
        "plt.plot(X_test_1d, ground_truth_1d(X_test_1d.ravel()), 'k--', linewidth=2, label='True Function')\n",
        "plt.scatter(X_train_1d, y_train_1d, c='red', s=200, zorder=10, edgecolors='black', linewidths=2, label='Training Data')\n",
        "plt.xlabel('x', fontsize=12)\n",
        "plt.ylabel('y', fontsize=12)\n",
        "plt.title('Gaussian Process: Predictions with Uncertainty', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Observations:\")\n",
        "print(\"  - Uncertainty is LOW near training points (shaded area is narrow)\")\n",
        "print(\"  - Uncertainty is HIGH far from training points (shaded area is wide)\")\n",
        "print(\"  - GP 'knows what it doesn't know'!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "### Uncertainty as a Sampling Strategy\n",
        "\n",
        "If we could only sample ONE more point, where should it be?\n",
        "\n",
        "Let's find the point with maximum uncertainty:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find point with maximum uncertainty\n",
        "max_uncertainty_idx = np.argmax(y_std)\n",
        "x_next = X_test_1d[max_uncertainty_idx]\n",
        "y_next = ground_truth_1d(x_next.ravel())\n",
        "\n",
        "print(f\"Most uncertain point: x = {x_next[0]:.3f}\")\n",
        "print(f\"Uncertainty (std): {y_std[max_uncertainty_idx]:.3f}\")\n",
        "print(f\"\\nThis is the BEST point to sample next!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "## Part 3: AutoRA Uncertainty Experimentalist\n",
        "\n",
        "Now let's apply uncertainty sampling to our 2AFC experiment using AutoRA's built-in uncertainty experimentalist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "from resources.synthetic import twoafc\n",
        "from autora.state import StandardState, on_state, estimator_on_state\n",
        "from autora.experimentalist.random import random_sample\n",
        "\n",
        "# Define participant parameters\n",
        "n_units = 100\n",
        "parameters = np.random.normal(1, 0.5, (n_units, 2))\n",
        "parameters = np.where(parameters < 0, 0, parameters)\n",
        "\n",
        "# Create experiment\n",
        "experiment = twoafc(parameters, resolution=10)\n",
        "\n",
        "# Get variable names\n",
        "iv_names = [iv.name for iv in experiment.variables.independent_variables]\n",
        "dv_names = [dv.name for dv in experiment.variables.dependent_variables]\n",
        "\n",
        "print(\"Experiment setup complete!\")\n",
        "print(f\"IVs: {iv_names}\")\n",
        "print(f\"DVs: {dv_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "### Creating a GP-based Theorist\n",
        "\n",
        "We'll use a Gaussian Process Regressor that can handle multiple inputs (participant_id, ratio, scatteredness).\n",
        "\n",
        "**Important**: We need to create a wrapper that works with AutoRA's state system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create GP model with appropriate kernel\n",
        "# RBF kernel works well for smooth functions like our 2AFC experiment\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF([1.0, 1.0, 1.0], (1e-2, 1e2)) + WhiteKernel(noise_level=0.01)\n",
        "gp_model = GaussianProcessRegressor(\n",
        "    kernel=kernel, \n",
        "    n_restarts_optimizer=5, \n",
        "    random_state=42,\n",
        "    normalize_y=True  # Normalize target values for better numerical stability\n",
        ")\n",
        "\n",
        "print(\"GP model created!\")\n",
        "print(f\"Kernel: {gp_model.kernel}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "### Installing and Using AutoRA's Uncertainty Experimentalist\n",
        "\n",
        "First, make sure you have the uncertainty experimentalist installed:\n",
        "\n",
        "```bash\n",
        "pip install -U \"autora[experimentalist-uncertainty]\"\n",
        "```\n",
        "\n",
        "Now let's import and set it up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import uncertainty experimentalist\n",
        "try:\n",
        "    from autora.experimentalist.uncertainty import uncertainty_sample\n",
        "    print(\"✓ Uncertainty experimentalist imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(\"✗ Error importing uncertainty experimentalist.\")\n",
        "    print(\"  Please install with: pip install -U 'autora[experimentalist-uncertainty]'\")\n",
        "    raise e\n",
        "\n",
        "# The uncertainty experimentalist needs:\n",
        "# 1. A pool of candidate conditions to choose from\n",
        "# 2. A model that supports predict(..., return_std=True) - like GP\n",
        "# 3. Number of samples to select\n",
        "\n",
        "print(\"\\nUncertainty experimentalist ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {},
      "source": [
        "## Part 4: Comparison Experiment - Random vs. Uncertainty Sampling\n",
        "\n",
        "Let's run a proper comparison:\n",
        "- **Strategy 1**: Random sampling (baseline)\n",
        "- **Strategy 2**: Uncertainty sampling (intelligent selection)\n",
        "\n",
        "We'll run 10 cycles and track model performance over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "### Strategy 1: Random Sampling Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrap components for state operations\n",
        "experiment_runner = on_state(experiment.run, output=['experiment_data'])\n",
        "experimentalist_random = on_state(random_sample, output=['conditions'])\n",
        "\n",
        "# Create fresh GP for random strategy\n",
        "kernel_random = C(1.0, (1e-3, 1e3)) * RBF([1.0, 1.0, 1.0], (1e-2, 1e2)) + WhiteKernel(noise_level=0.01)\n",
        "gp_random = GaussianProcessRegressor(kernel=kernel_random, n_restarts_optimizer=5, random_state=42, normalize_y=True)\n",
        "\n",
        "theorist_random = estimator_on_state(gp_random)\n",
        "\n",
        "# Initialize state\n",
        "state_random = StandardState(\n",
        "    variables=experiment.variables,\n",
        "    conditions=pd.DataFrame(columns=iv_names),\n",
        "    experiment_data=pd.DataFrame(columns=iv_names + dv_names),\n",
        "    models=[gp_random]\n",
        ")\n",
        "\n",
        "# Run 10 cycles\n",
        "n_cycles = 10\n",
        "samples_per_cycle = 5  # Sample 5 conditions per participant per cycle\n",
        "mse_history_random = []\n",
        "\n",
        "print(\"Running RANDOM sampling strategy...\\n\")\n",
        "\n",
        "for cycle in range(n_cycles):\n",
        "    # 1. Propose conditions randomly\n",
        "    state_random = experimentalist_random(\n",
        "        state_random, \n",
        "        num_samples=samples_per_cycle,\n",
        "        random_state=42+cycle,\n",
        "        sample_all=['participant_id']\n",
        "    )\n",
        "    \n",
        "    # 2. Run experiment\n",
        "    state_random = experiment_runner(state_random, added_noise=0.0, random_state=42+cycle)\n",
        "    \n",
        "    # 3. Train model\n",
        "    state_random = theorist_random(state_random)\n",
        "    \n",
        "    # 4. Evaluate\n",
        "    X = state_random.experiment_data[iv_names].values\n",
        "    y_true = state_random.experiment_data[dv_names].values.ravel()\n",
        "    y_pred = state_random.models[0].predict(X)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mse_history_random.append(mse)\n",
        "    \n",
        "    print(f\"Cycle {cycle+1:2d}/{n_cycles}: {len(state_random.experiment_data):4d} samples, MSE = {mse:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Random sampling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "### Strategy 2: Uncertainty Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "from autora.experimentalist.pooler import grid_pool\n",
        "\n",
        "# Wrap uncertainty experimentalist for state operations\n",
        "experimentalist_uncertainty = on_state(uncertainty_sample, output=['conditions'])\n",
        "pool_generator = on_state(grid_pool, output=['conditions'])  # Create pool of candidates\n",
        "\n",
        "# Create fresh GP for uncertainty strategy\n",
        "kernel_uncertainty = C(1.0, (1e-3, 1e3)) * RBF([1.0, 1.0, 1.0], (1e-2, 1e2)) + WhiteKernel(noise_level=0.01)\n",
        "gp_uncertainty = GaussianProcessRegressor(kernel=kernel_uncertainty, n_restarts_optimizer=5, random_state=42, normalize_y=True)\n",
        "\n",
        "theorist_uncertainty = estimator_on_state(gp_uncertainty)\n",
        "\n",
        "# Initialize state with a few random samples (seed)\n",
        "seed_conditions = random_sample(experiment.variables, num_samples=2, random_state=42, sample_all=['participant_id'])\n",
        "state_uncertainty = StandardState(\n",
        "    variables=experiment.variables,\n",
        "    conditions=seed_conditions,\n",
        "    experiment_data=pd.DataFrame(columns=iv_names + dv_names),\n",
        "    models=[gp_uncertainty]\n",
        ")\n",
        "\n",
        "# Run initial experiment with seed conditions\n",
        "state_uncertainty = experiment_runner(state_uncertainty, added_noise=0.0, random_state=42)\n",
        "state_uncertainty = theorist_uncertainty(state_uncertainty)\n",
        "\n",
        "# Track history\n",
        "mse_history_uncertainty = []\n",
        "\n",
        "# Evaluate initial state\n",
        "X = state_uncertainty.experiment_data[iv_names].values\n",
        "y_true = state_uncertainty.experiment_data[dv_names].values.ravel()\n",
        "y_pred = state_uncertainty.models[0].predict(X)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "mse_history_uncertainty.append(mse)\n",
        "\n",
        "print(\"Running UNCERTAINTY sampling strategy...\\n\")\n",
        "print(f\"Cycle  0/{n_cycles}: {len(state_uncertainty.experiment_data):4d} samples (seed), MSE = {mse:.4f}\")\n",
        "\n",
        "for cycle in range(1, n_cycles):\n",
        "    # 1. Generate pool of candidate conditions\n",
        "    pool_state = StandardState(\n",
        "        variables=experiment.variables,\n",
        "        conditions=pd.DataFrame(columns=iv_names),\n",
        "        experiment_data=state_uncertainty.experiment_data.copy(),\n",
        "        models=state_uncertainty.models\n",
        "    )\n",
        "    pool_state = pool_generator(pool_state, num_samples=20, sample_all=['participant_id'])\n",
        "    \n",
        "    # 2. Select most uncertain conditions from pool\n",
        "    pool_state = experimentalist_uncertainty(pool_state, num_samples=samples_per_cycle)\n",
        "    \n",
        "    # 3. Update main state with selected conditions\n",
        "    state_uncertainty.conditions = pool_state.conditions\n",
        "    \n",
        "    # 4. Run experiment\n",
        "    state_uncertainty = experiment_runner(state_uncertainty, added_noise=0.0, random_state=42+cycle)\n",
        "    \n",
        "    # 5. Train model\n",
        "    state_uncertainty = theorist_uncertainty(state_uncertainty)\n",
        "    \n",
        "    # 6. Evaluate\n",
        "    X = state_uncertainty.experiment_data[iv_names].values\n",
        "    y_true = state_uncertainty.experiment_data[dv_names].values.ravel()\n",
        "    y_pred = state_uncertainty.models[0].predict(X)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mse_history_uncertainty.append(mse)\n",
        "    \n",
        "    print(f\"Cycle {cycle:2d}/{n_cycles}: {len(state_uncertainty.experiment_data):4d} samples, MSE = {mse:.4f}\")\n",
        "\n",
        "print(\"\\n✓ Uncertainty sampling complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {},
      "source": [
        "## Part 5: Analysis - Comparing Strategies\n",
        "\n",
        "Let's visualize and analyze the results!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {},
      "source": [
        "### MSE Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_cycles+1), mse_history_random, 'o-', label='Random Sampling', linewidth=2, markersize=8)\n",
        "plt.plot(range(1, n_cycles+1), mse_history_uncertainty, 's-', label='Uncertainty Sampling', linewidth=2, markersize=8)\n",
        "plt.xlabel('Cycle', fontsize=12)\n",
        "plt.ylabel('Mean Squared Error', fontsize=12)\n",
        "plt.title('Learning Efficiency: Random vs. Uncertainty Sampling', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.yscale('log')  # Log scale to see differences more clearly\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFinal Performance:\")\n",
        "print(f\"  Random Sampling:      MSE = {mse_history_random[-1]:.4f}\")\n",
        "print(f\"  Uncertainty Sampling: MSE = {mse_history_uncertainty[-1]:.4f}\")\n",
        "improvement = (mse_history_random[-1] - mse_history_uncertainty[-1]) / mse_history_random[-1] * 100\n",
        "print(f\"\\n  Improvement: {improvement:.1f}% better with uncertainty sampling!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {},
      "source": [
        "### Sample Distribution - Where Did Each Strategy Sample?\n",
        "\n",
        "Let's visualize where each strategy chose to sample for one participant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get samples for participant 0\n",
        "participant_id = 0\n",
        "random_samples = state_random.experiment_data[state_random.experiment_data['participant_id'] == participant_id]\n",
        "uncertainty_samples = state_uncertainty.experiment_data[state_uncertainty.experiment_data['participant_id'] == participant_id]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Random sampling\n",
        "scatter1 = axes[0].scatter(\n",
        "    random_samples['ratio'], \n",
        "    random_samples['scatteredness'], \n",
        "    c=range(len(random_samples)),\n",
        "    cmap='viridis',\n",
        "    s=100,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidths=1\n",
        ")\n",
        "axes[0].set_xlabel('Ratio', fontsize=12)\n",
        "axes[0].set_ylabel('Scatteredness', fontsize=12)\n",
        "axes[0].set_title(f'Random Sampling (Participant {participant_id})', fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xlim(-0.1, 1.1)\n",
        "axes[0].set_ylim(-0.1, 1.1)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Sample Order')\n",
        "\n",
        "# Uncertainty sampling\n",
        "scatter2 = axes[1].scatter(\n",
        "    uncertainty_samples['ratio'], \n",
        "    uncertainty_samples['scatteredness'], \n",
        "    c=range(len(uncertainty_samples)),\n",
        "    cmap='viridis',\n",
        "    s=100,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidths=1\n",
        ")\n",
        "axes[1].set_xlabel('Ratio', fontsize=12)\n",
        "axes[1].set_ylabel('Scatteredness', fontsize=12)\n",
        "axes[1].set_title(f'Uncertainty Sampling (Participant {participant_id})', fontsize=14)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xlim(-0.1, 1.1)\n",
        "axes[1].set_ylim(-0.1, 1.1)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Sample Order')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nSampling Coverage:\")\n",
        "print(f\"  Random: {len(random_samples)} samples\")\n",
        "print(f\"  Uncertainty: {len(uncertainty_samples)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {},
      "source": [
        "### Uncertainty Reduction Over Time\n",
        "\n",
        "Let's visualize how uncertainty changes as we collect more data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test grid for participant 0\n",
        "ratio_range = np.linspace(0, 1, 30)\n",
        "scatter_range = np.linspace(0, 1, 30)\n",
        "ratio_grid, scatter_grid = np.meshgrid(ratio_range, scatter_range)\n",
        "X_grid = np.c_[\n",
        "    np.full(ratio_grid.size, participant_id),  # participant_id\n",
        "    ratio_grid.ravel(),  # ratio\n",
        "    scatter_grid.ravel()  # scatteredness\n",
        "]\n",
        "\n",
        "# Get uncertainty from both models\n",
        "_, std_random = state_random.models[0].predict(X_grid, return_std=True)\n",
        "_, std_uncertainty = state_uncertainty.models[0].predict(X_grid, return_std=True)\n",
        "\n",
        "# Reshape for plotting\n",
        "std_random_grid = std_random.reshape(ratio_grid.shape)\n",
        "std_uncertainty_grid = std_uncertainty.reshape(ratio_grid.shape)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Random\n",
        "im1 = axes[0].contourf(ratio_grid, scatter_grid, std_random_grid, levels=20, cmap='YlOrRd')\n",
        "axes[0].scatter(random_samples['ratio'], random_samples['scatteredness'], \n",
        "                c='blue', s=50, alpha=0.7, edgecolors='black', linewidths=1, label='Sampled Points')\n",
        "axes[0].set_xlabel('Ratio', fontsize=12)\n",
        "axes[0].set_ylabel('Scatteredness', fontsize=12)\n",
        "axes[0].set_title(f'Uncertainty Map: Random Sampling', fontsize=14)\n",
        "axes[0].legend()\n",
        "plt.colorbar(im1, ax=axes[0], label='Prediction Std Dev')\n",
        "\n",
        "# Uncertainty\n",
        "im2 = axes[1].contourf(ratio_grid, scatter_grid, std_uncertainty_grid, levels=20, cmap='YlOrRd')\n",
        "axes[1].scatter(uncertainty_samples['ratio'], uncertainty_samples['scatteredness'], \n",
        "                c='blue', s=50, alpha=0.7, edgecolors='black', linewidths=1, label='Sampled Points')\n",
        "axes[1].set_xlabel('Ratio', fontsize=12)\n",
        "axes[1].set_ylabel('Scatteredness', fontsize=12)\n",
        "axes[1].set_title(f'Uncertainty Map: Uncertainty Sampling', fontsize=14)\n",
        "axes[1].legend()\n",
        "plt.colorbar(im2, ax=axes[1], label='Prediction Std Dev')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"  - Darker red = higher uncertainty\")\n",
        "print(\"  - Uncertainty sampling focuses on high-uncertainty regions\")\n",
        "print(\"  - This leads to more uniform uncertainty reduction!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-28",
      "metadata": {},
      "source": [
        "## Part 6: Information-Theoretic Analysis\n",
        "\n",
        "Let's connect back to information theory by computing mutual information over time.\n",
        "\n",
        "We'll approximate mutual information using the uncertainty reduction:\n",
        "$$I(X; Y) \\approx \\text{Initial Entropy} - \\text{Final Entropy}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-29",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_avg_uncertainty(model, X_test):\n",
        "    \"\"\"Compute average prediction uncertainty over test set\"\"\"\n",
        "    _, std = model.predict(X_test, return_std=True)\n",
        "    return np.mean(std)\n",
        "\n",
        "def uncertainty_to_entropy(std):\n",
        "    \"\"\"Convert Gaussian std to differential entropy\"\"\"\n",
        "    # For Gaussian: H = 0.5 * log(2 * pi * e * sigma^2)\n",
        "    return 0.5 * np.log(2 * np.pi * np.e * std**2)\n",
        "\n",
        "# Compute average uncertainty on full grid\n",
        "avg_uncertainty_random = compute_avg_uncertainty(state_random.models[0], X_grid)\n",
        "avg_uncertainty_uncertainty = compute_avg_uncertainty(state_uncertainty.models[0], X_grid)\n",
        "\n",
        "# Convert to entropy\n",
        "entropy_random = uncertainty_to_entropy(avg_uncertainty_random)\n",
        "entropy_uncertainty = uncertainty_to_entropy(avg_uncertainty_uncertainty)\n",
        "\n",
        "print(\"\\nInformation-Theoretic Analysis:\")\n",
        "print(\"\\nAverage Prediction Uncertainty (over design space):\")\n",
        "print(f\"  Random Sampling:      {avg_uncertainty_random:.4f}\")\n",
        "print(f\"  Uncertainty Sampling: {avg_uncertainty_uncertainty:.4f}\")\n",
        "print(f\"  → Reduction: {(1 - avg_uncertainty_uncertainty/avg_uncertainty_random)*100:.1f}%\")\n",
        "\n",
        "print(\"\\nDifferential Entropy (nats):\")\n",
        "print(f\"  Random Sampling:      {entropy_random:.4f}\")\n",
        "print(f\"  Uncertainty Sampling: {entropy_uncertainty:.4f}\")\n",
        "print(f\"  → Lower entropy = less remaining uncertainty!\")\n",
        "\n",
        "# Visualize entropy reduction\n",
        "plt.figure(figsize=(8, 6))\n",
        "strategies = ['Random\\nSampling', 'Uncertainty\\nSampling']\n",
        "entropies = [entropy_random, entropy_uncertainty]\n",
        "colors = ['#ff7f0e', '#2ca02c']\n",
        "\n",
        "bars = plt.bar(strategies, entropies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "plt.ylabel('Differential Entropy (nats)', fontsize=12)\n",
        "plt.title('Remaining Uncertainty After 10 Cycles', fontsize=14)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, entropy in zip(bars, entropies):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{entropy:.3f}',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-30",
      "metadata": {},
      "source": [
        "## Summary & Key Takeaways\n",
        "\n",
        "You've learned:\n",
        "\n",
        "1. ✅ **Information Theory Recap**: Entropy measures uncertainty, mutual information measures learning\n",
        "2. ✅ **Gaussian Processes**: Models that provide natural uncertainty estimates via $\\sigma(x)$\n",
        "3. ✅ **Uncertainty Sampling**: Query where the model is most uncertain\n",
        "4. ✅ **AutoRA Implementation**: Using `uncertainty_sample` experimentalist with GP models\n",
        "5. ✅ **Performance Gains**: Uncertainty sampling learns faster than random sampling\n",
        "6. ✅ **Information-Theoretic Analysis**: Measuring entropy reduction and information gain\n",
        "\n",
        "### Key Insight\n",
        "\n",
        "**Not all samples are equally valuable!**\n",
        "\n",
        "By intelligently selecting samples where we're most uncertain, we:\n",
        "- Reduce prediction error faster\n",
        "- Cover the design space more efficiently\n",
        "- Maximize information gain per observation\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "In the next tutorial (**autora_advanced.ipynb**), you'll learn:\n",
        "- **Model Disagreement**: Using ensembles for even smarter sampling\n",
        "- **Query-by-Committee**: Selecting samples where models disagree most\n",
        "- **Combining Strategies**: Hybrid approaches for robust active learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-31",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. **Different Kernels**: Try different GP kernels (Matern, RationalQuadratic). How does this affect uncertainty estimates?\n",
        "\n",
        "2. **Sample Budget**: Run experiments with different `samples_per_cycle` (1, 3, 10). When does uncertainty sampling show the biggest advantage?\n",
        "\n",
        "3. **Noise Levels**: Add noise to observations (`added_noise=0.1, 0.5`). How robust is uncertainty sampling to noise?\n",
        "\n",
        "4. **Pool Size**: Change the pool size in `grid_pool(num_samples=...)`. Does a larger pool improve performance?\n",
        "\n",
        "5. **Multi-Participant**: Analyze uncertainty patterns across different participants. Are some participants harder to model?\n",
        "\n",
        "6. **Mutual Information**: Implement a more sophisticated MI estimator using differential entropy of the GP posterior. Track MI over cycles.\n",
        "\n",
        "7. **Explore-Exploit**: Modify the experimentalist to balance uncertainty sampling (explore) with sampling near optimal points (exploit)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-32",
      "metadata": {},
      "source": [
        "## Congratulations!\n",
        "\n",
        "You've mastered uncertainty-based active learning! You now understand how to use model uncertainty to intelligently guide experimental design, and you can connect these methods to information-theoretic principles.\n",
        "\n",
        "Next up: **Model Disagreement** for even more sophisticated active learning strategies!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
