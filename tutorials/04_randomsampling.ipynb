{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Random Sampling Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will learn about different random sampling strategies and in which cases they are useful.\n",
    "\n",
    "This notebook covers the following strategies:\n",
    "- sampling from a normal distribution\n",
    "- sampling from a uniform distribution\n",
    "- sampling with a latin hypercube\n",
    "\n",
    "You will explore the generated combinations of conditions and compare them visually.\n",
    "\n",
    "You are also going to scale the gathered conditions to a certain value range to keep the differently collected conditions as comparable as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we set the general parameters and methods which will be used at each stage of this notebook.\n",
    "\n",
    "The parameters in this case are:\n",
    "- the sample size\n",
    "- the number of factors.\n",
    "- the value range for the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 10\n",
    "n_factors = 2\n",
    "level_range = (0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Before gathering the different conditions, we will implement a scaling method.\n",
    "\n",
    "This scaling method will scale the gathered conditions to a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    Scale the input to the range (0, 1).\n",
    "    Args:\n",
    "        x (np.ndarray): the input to scale. Can be any type that can be converted to a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    #  add your code here\n",
    "    x_max = np.max(x)\n",
    "    x_min = np.min(x)\n",
    "    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work properly? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "array = scale(array)\n",
    "\n",
    "print(f\"The scaled array is {array}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a normal distribution\n",
    "\n",
    "The first sampling strategy which we will implement is sampling from a normal distribution.\n",
    "\n",
    "We take the numpy.random.normal method.\n",
    "\n",
    "You can take a look at the [official documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html), if you are not familiar with it.\n",
    "\n",
    "Use the parameters from the Setup section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the conditions combinations with a normal distribution\n",
    "# PS: use the size argument to define the number of obtained samples\n",
    "# add your code here\n",
    "conditions_normal = np.random.normal(loc=0, scale=1, size=(100, n_factors))\n",
    "\n",
    "# scale the conditions combinations to the range (0, 1) with your scale function\n",
    "# add your code here\n",
    "conditions_normal = scale(conditions_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(conditions_normal[:, 0], conditions_normal[:, 1])\n",
    "plt.title('Random sampling from a normal distribution')\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Scatteredness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered conditions change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a uniform distribution\n",
    "\n",
    "The second sampling strategy which we will implement is sampling from a uniform distribution.\n",
    "\n",
    "We take the numpy.random.normal method.\n",
    "\n",
    "You can take a look at the [official documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html), if you are not familiar with it.\n",
    "\n",
    "Use the parameters from the Setup section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the conditions combinations with a uniform distribution\n",
    "# add your code here\n",
    "conditions_uniform = np.random.uniform(0, 1, (100, n_factors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(conditions_uniform[:, 0], conditions_uniform[:, 1])\n",
    "plt.title('Random sampling from a uniform distribution')\n",
    "plt.xlabel('Ratio')\n",
    "plt.ylabel('Scatteredness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered conditions change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from a Latin Hypercube\n",
    "\n",
    "Lastly, we will implement the latin hypercube strategy.\n",
    "\n",
    "For that we will take the scipy.stats.qmc.LatinHypercube method.\n",
    "\n",
    "Please make yourself familiar with it through the [official documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html).\n",
    "\n",
    "And here again - use the parameters from the setup.\n",
    "\n",
    "Tip: We import the LatinHypercube class from scipy. Use the 'random' method of this class to create the condition combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.qmc import LatinHypercube\n",
    "\n",
    "# generate the conditions combinations with a Latin Hypercube\n",
    "latinhypercube = LatinHypercube(d=n_factors)\n",
    "conditions_lhc = latinhypercube.random(100)\n",
    "\n",
    "# scale the conditions combinations to the range (0, 1) with your scale function\n",
    "conditions_lhc = scale(conditions_lhc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Let's inspect the result in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(conditions_lhc[:, 0], conditions_lhc[:, 1])\n",
    "plt.title('Random sampling from a Latin Hypercube')\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the result look like?\n",
    "\n",
    "Which areas were covered well and which less?\n",
    "\n",
    "Does it look any different from the normal distribution?\n",
    "\n",
    "Increase the sample size and re-run the code to see how the gathered conditions change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed analysis of the obtained results\n",
    "\n",
    "By just looking at the plots the sampling techniques and their differences can be difficult to identify.\n",
    "\n",
    "Here we will now analyze the obtained results with respect to their coverage.\n",
    "\n",
    "We will compute the coverage and plot a 2D histogram of the differently gathered conditions. \n",
    "\n",
    "This will give us a better idea of the differences than simple scatter plots. Especially between uniform and Latin Hypercube sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, copy the sampling commands which you already implemented into the following cell so we do not have to re-run each cell when changing the sample size.\n",
    "\n",
    "(And don't forget to scale here as well)\n",
    "\n",
    "For the Latin Hypercube:\n",
    "    Make this time use of the `optimization` keyword (value: `lloyd`) and watch the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "np.random.seed(0)\n",
    "\n",
    "conditions_normal = scale(np.random.normal(0, 1, (n_samples, 2)))\n",
    "conditions_uniform = np.random.uniform(0, 1, (n_samples, 2))\n",
    "conditions_lhc = scale(LatinHypercube(n_factors, optimization='lloyd').random(n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the coverage of the conditions with respect to the design space!\n",
    "\n",
    "The steps for our analysis look as follows:\n",
    "1. Compute the target coverage which is $target=\\frac{n_\\text{samples}}{n_\\text{bins}**n_\\text{factors}}$\n",
    "2. Compute 2D Histograms for each of the methods where the number of samples per bin is counted\n",
    "3. Compute the bin-wide coverage for each method\n",
    "4. Compute under-, over- and highly over-represented bins ($bin_\\text{count}<target$; $bin_\\text{count}>target$; $bin_\\text{count}>target*threshold$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_bins = 10\n",
    "threshold_highly_overrepresented = 2  # more than 2 samples in one bin\n",
    "\n",
    "# compute the target coverage\n",
    "target_coverage = np.max((n_samples/(hist_bins**n_factors), 1))\n",
    "threshold_highly_overrepresented = target_coverage * threshold_highly_overrepresented\n",
    "print(f'Target coverage: {target_coverage} samples per bin')\n",
    "\n",
    "# compute 2D histograms for each sampling method with the numpy method\n",
    "hist_normal = np.histogram2d(conditions_normal[:, 0], conditions_normal[:, 1], bins=hist_bins)[0]\n",
    "hist_uniform = np.histogram2d(conditions_uniform[:, 0], conditions_uniform[:, 1], bins=hist_bins)[0]\n",
    "hist_lhc = np.histogram2d(conditions_lhc[:, 0], conditions_lhc[:, 1], bins=hist_bins)[0]\n",
    "\n",
    "# compute the actual coverage for each sampling method\n",
    "coverage_normal = np.round(np.sum(hist_normal > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "coverage_uniform = np.round(np.sum(hist_uniform > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "coverage_lhc = np.round(np.sum(hist_lhc > 0)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of underrepresented bins for each sampling method\n",
    "underrep_normal = round(np.sum(hist_normal < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "underrep_uniform = round(np.sum(hist_uniform < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "underrep_lhc = round(np.sum(hist_lhc < target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of overrepresented bins for each sampling method\n",
    "overrep_normal = round(np.sum(hist_normal > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "overrep_uniform = round(np.sum(hist_uniform > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "overrep_lhc = round(np.sum(hist_lhc > target_coverage)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# compute the amount of highly overrepresented bins for each sampling method\n",
    "highly_overrep_normal = round(np.sum(hist_normal > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "highly_overrep_uniform = round(np.sum(hist_uniform > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "highly_overrep_lhc = round(np.sum(hist_lhc > threshold_highly_overrepresented)/(hist_bins**n_factors) * 100, 2)\n",
    "\n",
    "# print the results in a table\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n",
    "print(f'{\"Sampling method\":<25} {\"Coverage [%]\":<15} {\"Underrepresented [%]\":<25} {\"Overrepresented [%]\":<25} {\"Highly overrepresented [%]\":<25}')\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n",
    "print(f'{\"Normal distribution\":<25} {coverage_normal:<15} {underrep_normal:<25} {overrep_normal:<25} {highly_overrep_normal:<25}')\n",
    "print(f'{\"Uniform distribution\":<25} {coverage_uniform:<15} {underrep_uniform:<25} {overrep_uniform:<25} {highly_overrep_uniform:<25}')\n",
    "print(f'{\"Latin Hypercube\":<25} {coverage_lhc:<15} {underrep_lhc:<25} {overrep_lhc:<25} {highly_overrep_lhc:<25}')\n",
    "print(f'{\"-\"*25:<25} {\"-\"*15:<15} {\"-\"*25:<25} {\"-\"*25:<25} {\"-\"*25:<25}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the corresponding plots!\n",
    "\n",
    "The plotting is already implemented to make your life a little bit easier. \n",
    "\n",
    "You are still very welcome to go through the plotting code on your own as oftentimes this can be a quite challenging part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter = True\n",
    "\n",
    "# make a figure with three subplots for the three sampling methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# plot the samples from the normal distribution on the first subplot\n",
    "axes[0].hist2d(conditions_normal[:, 0], conditions_normal[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[0].scatter(conditions_normal[:, 0], conditions_normal[:, 1], color='orange')\n",
    "axes[0].set_title('normal distribution')\n",
    "axes[0].set_xlabel('Factor 1')\n",
    "axes[0].set_ylabel('Factor 2')\n",
    "\n",
    "# plot the samples from the uniform distribution on the second subplot\n",
    "axes[1].hist2d(conditions_uniform[:, 0], conditions_uniform[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[1].scatter(conditions_uniform[:, 0], conditions_uniform[:, 1], color='orange')\n",
    "axes[1].set_title('uniform distribution')\n",
    "axes[1].set_xlabel('Factor 1')\n",
    "\n",
    "# plot the LHS samples on the third subplot\n",
    "axes[2].hist2d(conditions_lhc[:, 0], conditions_lhc[:, 1], bins=hist_bins, cmap='Blues')\n",
    "if scatter:\n",
    "    axes[2].scatter(conditions_lhc[:, 0], conditions_lhc[:, 1], color='orange')\n",
    "axes[2].set_title('Latin Hypercube')\n",
    "axes[2].set_xlabel('Factor 1')\n",
    "\n",
    "# set figure title\n",
    "fig.suptitle('Sampling from')\n",
    "\n",
    "# share y axis\n",
    "axes[1].sharey(axes[0])\n",
    "axes[2].sharey(axes[0])\n",
    "# hide y axis for the second and third subplot\n",
    "axes[1].get_yaxis().set_visible(False)\n",
    "axes[2].get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing all these analysis: What's your impression of the different random sampling methods?\n",
    "\n",
    "Can you think of use-cases for each of the method?\n",
    "\n",
    "Can you tell now a difference between the uniform and Latin Hypercube sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's set up our own experiment with these sampling methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we get different results for the presented random sampling methods with respect to noise, dataset size and dataset structure (i.e. number of repetitions per condition vs number of conditions).\n",
    "First of all, we have to import the existing classes for the experimental unit, the dataset generator, and the feed-forward-network regressor. You should be familiar with these from the last tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library, package and class imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "# from skorch import NeuralNetRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the custom classes for our experiment\n",
    "from resources.synthetic import experimental_unit, generate_dataset, cognitive_model\n",
    "from resources.regressors import FFN, FFNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to set up the sampling strategy and generate the dataset. You can use the respective parts from the first tutorial. Set the number of participants, tested conditions, repitions and the noise level as seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset parameters\n",
    "\n",
    "# number of experimental units\n",
    "n_units = 100\n",
    "n_parameters_per_unit = 2\n",
    "\n",
    "# number of observations per experimental unit\n",
    "n_conditions = 100\n",
    "n_factors = 2\n",
    "\n",
    "# number of repetitions per condition\n",
    "n_repetitions = 1\n",
    "\n",
    "# amount of noise which we are going to add to the data\n",
    "noise_level = 0.3\n",
    "\n",
    "# relative amount of train vs test samples\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_units = None\n",
    "dataset_train = None\n",
    "dataset_test = None\n",
    "parameters = None\n",
    "conditions = None\n",
    "\n",
    "# create parameters from a normal distribution \n",
    "# w.r.t to human participants this is a more natural distribution than e.g. an uniform distribution\n",
    "parameters = scale(np.random.normal(0, 0.5, (n_units, n_parameters_per_unit))) + 1\n",
    "\n",
    "# draw conditions based on your sampling strategy of choice, i.e. normal, uniform, latin hypercube and scale them if necessary to the range (0, 1)\n",
    "conditions = np.random.uniform(0, 1, (100, 2))\n",
    "\n",
    "# create a list of experimental units and fill it with parameterized units\n",
    "experimental_units = []\n",
    "\n",
    "# create the experimental units\n",
    "for i in range(n_units):\n",
    "    experimental_units.append(\n",
    "        experimental_unit(\n",
    "            cognitive_model=cognitive_model,\n",
    "            parameters=parameters[i],\n",
    "            noise_level=noise_level,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# you can use the generate_dataset method to create a dataset which you can then split into train and test samples\n",
    "dataset_train, dataset_test = generate_dataset(experimental_units, conditions, train_ratio=train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has the features `(unit id, ratio, scatteredness, response time)`\n",
    "\n",
    "Let's take a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_train))\n",
    "print(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_test))\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the regressor based on the generated dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNRegressor(\n",
    "    FFN(n_units, 2), \n",
    "    max_epochs=100, \n",
    "    lr=0.1,\n",
    "    )\n",
    "\n",
    "model.fit(\n",
    "    X=dataset_train[:, 0:-1], \n",
    "    y=dataset_train[:, -1][:, None],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our trained model performs on the test data!\n",
    "\n",
    "Remember the test data are held-out observations from our full dataset which we are now using to evaluate how well our model generalizes.\n",
    "\n",
    "This is important because a model could also simply learn the training samples \"by heart\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(dataset_test[:, 0:-1])\n",
    "loss = mean_squared_error(dataset_test[:, -1][:, None], prediction)\n",
    "\n",
    "print(f\"Test loss: {loss:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the trained model in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id of the unit which we want to visualize\n",
    "unit_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the recovered response times over the conditions\n",
    "# define the factor levels\n",
    "x = np.linspace(0, 1)\n",
    "y = np.linspace(0, 1)\n",
    "x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "sample_size = len(x)\n",
    "\n",
    "# initiate the z array\n",
    "z_recovered = np.zeros((sample_size, sample_size))\n",
    "z_real = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# collect the observations\n",
    "for i in range(sample_size):\n",
    "    condition = torch.tensor(np.stack((x_mesh[i], y_mesh[i]), axis=-1))\n",
    "    unit_id_array = torch.full((condition.shape[0], 1), unit_id)\n",
    "    X = torch.cat((unit_id_array, condition), axis=-1)\n",
    "    \n",
    "    z_recovered[i, :] = model.predict(X).reshape(-1)\n",
    "    z_real[i, :] = experimental_units[unit_id].step(x_mesh[i], y_mesh[i], noise=False)\n",
    "    \n",
    "# make a surface plot to visualize the ground_truth\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(x_mesh, y_mesh, z_recovered, cmap=cm.Reds, alpha=0.5)\n",
    "ax.plot_surface(x_mesh, y_mesh, z_real, cmap=cm.Blues, alpha=0.5)\n",
    "ax.set_title('Real (blue) vs recovered (red) response times by a neural network over all factor levels')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatterdness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the model fit well? Try another random sampling algorithm and see how that changes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've finished the tutorial on random sampling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
