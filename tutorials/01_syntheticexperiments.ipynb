{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to synthetic experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the concept of synthetic experiments and demonstrates how to implement one using Python.\n",
    "\n",
    "Synthetic experiments are a powerful tool for studying the relationship between experimental conditions (e.g., ratio and scatteredness in our 2AFC task) and observations (e.g., response times) in a controlled environment. They allow us to collect synthetic data that resembles real empirical data but is less complex, only as noisy as we want it to be, and free from real-world limitations such as financial and time restrictions.\n",
    "\n",
    "Synthetic experiments use cognitive models instead of real participants. In cognitive science, this means replacing human participants with mathematical models that produce similar (but simpler) behavior. A cognitive model can be as simple as a single mathematical expression, making it much easier to evaluate our experimental designs and adaptive algorithms.\n",
    "\n",
    "This way, the concepts and algorithms from the 'Optimizing Experimental Design' course can be implemented, tested, and validated easily before running real experiments.\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "\n",
    "- Implement a cognitive model that simulates how response times depend on experimental conditions (ratio and scatteredness).\n",
    "- Understand how model parameters represent individual differences between synthetic participants.\n",
    "- Generate realistic data by adding measurement noise to the observations.\n",
    "- Collect a dataset from multiple synthetic participants tested on different conditions.\n",
    "- Fit statistical models to recover the cognitive model parameters from the data.\n",
    "- Understand how sample size and noise level affect parameter recovery quality.\n",
    "\n",
    "By the end of this tutorial, you will have a good understanding of how to implement and analyze synthetic experiments using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment description\n",
    "\n",
    "We want to examine the response times of participants in a two-alternative-forced-choice (2AFC) experiment. This experiment consists of an image which is shown for a short period of time. The image is a grid of either orange or blue tiles as shown in the picture below. \n",
    "\n",
    "We can control the experiment via the two factors **ratio** and **scatterdness**. \n",
    "**Ratio** determines the amount of blue vs orange tiles, where 0 means that the participant sees only orange tiles while 1 means that the amount of blue vs orange tiles is perfectly balanced. \n",
    "**Scatterdeness** determines how noisy the image appears, where 0 means that all orange tiles are placed on the left half while 1 means that the tiles are placed completely randomly.\n",
    "\n",
    "![static/img/2afc_grid.png](static/img/2afc_grid.png)\n",
    "\n",
    "(Image source: Trueblood J. S. et al (2021)., Urgency, Leakage, and the Relative Nature of Information Processing in\n",
    "Decision-Making.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the relevant libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add the path of the project folder to the python variables\n",
    "# That way python finds custom packages defined in the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a cognitive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement a cognitive model that simulates how participants respond in the 2AFC experiment. This model will generate response times based on the experimental conditions (ratio and scatteredness).\n",
    "\n",
    "The model includes **parameters** that represent individual differences between participants. For example, some participants might be generally faster or more sensitive to certain conditions than others. By varying these parameters, we can simulate different synthetic participants with different behavioral patterns.\n",
    "\n",
    "In this section, you will learn to:\n",
    "- Implement the cognitive model as a mathematical function that takes experimental conditions and participant parameters as input\n",
    "- Understand what the parameters represent cognitively (e.g., sensitivity to ratio vs. scatteredness)\n",
    "- Visualize how the model predicts response times across different experimental conditions\n",
    "- See how changing parameters creates different response patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the cognitive model\n",
    "\n",
    "The cognitive model is a mathematical function that takes the experimental conditions (ratio and scatteredness) as input and returns a predicted response time.\n",
    "\n",
    "Our model has two parameters:\n",
    "- **parameters[0]**: Controls sensitivity to the ratio (how balanced blue vs. orange tiles are)\n",
    "- **parameters[1]**: Controls sensitivity to scatteredness (how randomly distributed the tiles are)\n",
    "\n",
    "Different parameter values create different behavioral patterns, simulating individual differences between participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cognitive_model(ratio, scatteredness, parameters=np.ones(2,)):\n",
    "    \"\"\"This cognitive model predicts response times in the 2AFC task\n",
    "    \n",
    "    Args:\n",
    "        ratio (float): The balance of blue vs orange tiles (0 to 1)\n",
    "        scatteredness (float): How randomly distributed the tiles are (0 to 1)\n",
    "        parameters (Iterable[float], optional): Individual participant parameters. \n",
    "            parameters[0]: sensitivity to ratio\n",
    "            parameters[1]: sensitivity to scatteredness\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted response time\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is a bell-shaped function that can saturate\n",
    "    # Response time increases with ratio (more balanced = harder decision)\n",
    "    # Response time also increases with scatteredness (more scattered = harder to process)\n",
    "    response_time = (1 - np.exp(-np.power(ratio, 2) / parameters[0])) + np.power(scatteredness, parameters[1])\n",
    "    \n",
    "    return response_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your cognitive model by giving it different parameter values and check if it produces plausible response time patterns.\n",
    "\n",
    "You can modify the `parameters` variable to see how different participants might behave. For example:\n",
    "- Higher values of parameters[0] mean less sensitivity to ratio (flatter response to changes in blue/orange balance)\n",
    "- Higher values of parameters[1] mean stronger effect of scatteredness on response time\n",
    "\n",
    "This code visualizes the predicted response times across all combinations of ratio and scatteredness for one synthetic participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experimental conditions to test\n",
    "ratio = np.linspace(0, 1)\n",
    "scatteredness = np.linspace(0, 1)\n",
    "\n",
    "# Define parameters for one synthetic participant\n",
    "# Try changing these values to see how behavior changes!\n",
    "parameters = [0.5, 2]\n",
    "\n",
    "# DO NOT TOUCH THE CODE FROM HERE!\n",
    "\n",
    "# Set a sample size\n",
    "sample_size = len(ratio)\n",
    "\n",
    "# Initialize the response_time array\n",
    "response_time = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# Collect the predictions from the cognitive model\n",
    "ratio_mesh, scatteredness_mesh = np.meshgrid(ratio, scatteredness)\n",
    "for i in range(sample_size):\n",
    "    response_time[i, :] = cognitive_model(ratio_mesh[i], scatteredness_mesh[i], parameters)\n",
    "\n",
    "# Make a surface plot to visualize the cognitive model predictions\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(ratio_mesh, scatteredness_mesh, response_time, cmap=cm.Blues)\n",
    "ax.set_title('Cognitive Model: Response times across conditions')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatteredness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the pattern look plausible? \n",
    "\n",
    "Try different parameter configurations to get a feeling for how response times change from participant to participant. For instance:\n",
    "- Try parameters = [0.5, 1] vs. [2, 1] to see how sensitivity to ratio affects the pattern\n",
    "- Try parameters = [1, 0.5] vs. [1, 3] to see how sensitivity to scatteredness affects the pattern\n",
    "\n",
    "Afterwards you can move to the next part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding measurement noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real experiments always have measurement noise - random variability in the observations even when the experimental conditions are identical. This noise comes from many sources: momentary lapses in attention, motor variability, random fluctuations in the environment, etc.\n",
    "\n",
    "To make our synthetic data more realistic, we add random noise to each simulated response time. The noise is drawn from a normal distribution with mean 0, meaning it doesn't systematically bias the data up or down, just adds random variability.\n",
    "\n",
    "The **noise level** parameter controls how much variability we add:\n",
    "- noise_level = 0: Perfect, noise-free data (unrealistic)\n",
    "- noise_level = 0.1: Low noise (very controlled experiment)\n",
    "- noise_level = 0.5: Moderate noise (typical experiment)\n",
    "- noise_level = 1.0: High noise (difficult experimental conditions)\n",
    "\n",
    "We'll use numpy's `np.random.normal(mean, std)` function to generate random noise values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what noise looks like by generating a series of random noise samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.5\n",
    "\n",
    "# Generate 100 random noise samples\n",
    "noise_sample_size = 100\n",
    "noise_samples = np.random.normal(0, noise_level, noise_sample_size)\n",
    "    \n",
    "plt.plot(noise_samples, '.')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.title(f'Random noise samples (noise level = {noise_level})')\n",
    "plt.ylabel('Noise value')\n",
    "plt.xlabel('Sample number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add noise to the cognitive model predictions and see how it affects the response time pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set noise level\n",
    "noise_level = 0.2\n",
    "\n",
    "# Initialize the noisy response_time array\n",
    "response_time_noisy = np.zeros((sample_size, sample_size))\n",
    "\n",
    "# Collect the observations from the cognitive model\n",
    "for i in range(sample_size):\n",
    "    response_time_noisy[i, :] = cognitive_model(ratio_mesh[i], scatteredness_mesh[i], parameters)\n",
    "    \n",
    "# Add noise to each observation\n",
    "for i in range(response_time_noisy.shape[0]):\n",
    "    for j in range(response_time_noisy.shape[1]):\n",
    "        response_time_noisy[i, j] += np.random.normal(0, noise_level)\n",
    "\n",
    "# Make sure no response times are negative (response times must be positive)\n",
    "response_time_noisy = np.maximum(response_time_noisy, 0)\n",
    "\n",
    "# Compare clean vs. noisy predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": \"3d\"}, figsize=(14, 6))\n",
    "\n",
    "# Clean cognitive model\n",
    "ax1.plot_surface(ratio_mesh, scatteredness_mesh, response_time, cmap=cm.Blues)\n",
    "ax1.set_title('Clean cognitive model (no noise)')\n",
    "ax1.set_xlabel('Ratio')\n",
    "ax1.set_ylabel('Scatteredness')\n",
    "ax1.set_zlabel('Response time')\n",
    "\n",
    "# Noisy observations\n",
    "ax2.plot_surface(ratio_mesh, scatteredness_mesh, response_time_noisy, cmap=cm.Blues)\n",
    "ax2.set_title(f'With measurement noise (level = {noise_level})')\n",
    "ax2.set_xlabel('Ratio')\n",
    "ax2.set_ylabel('Scatteredness')\n",
    "ax2.set_zlabel('Response time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the observations change? Can you still see the underlying pattern? \n",
    "\n",
    "Try increasing the noise level gradually (e.g., 0.1, 0.5, 1.0, 2.0) to see how the signal-to-noise ratio decreases and the pattern becomes harder to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating synthetic participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already implemented a `SyntheticParticipant` class in the `resources.synthetic` module that combines the cognitive model and noise generation. This class represents a single synthetic participant with:\n",
    "- Their own cognitive model parameters (individual differences)\n",
    "- A specified noise level (measurement variability)\n",
    "\n",
    "Each synthetic participant can then be \"tested\" on different experimental conditions to generate simulated response times, just like testing a real participant in the lab.\n",
    "\n",
    "Let's import the class and create some synthetic participants with different parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two synthetic participants with different parameter values and compare their predicted response times across the same experimental conditions.\n",
    "\n",
    "Participant 1 will have parameters [1, 1] while Participant 2 will have parameters [2, 2], making them differentially sensitive to the experimental manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SyntheticParticipant class\n",
    "from resources.synthetic import experimental_unit as SyntheticParticipant\n",
    "\n",
    "# Define parameters for two different participants\n",
    "parameters_participant1 = [1, 1]\n",
    "parameters_participant2 = [2, 2]\n",
    "\n",
    "# Create two synthetic participants with different parameters\n",
    "participant1 = SyntheticParticipant(\n",
    "    cognitive_model=cognitive_model,\n",
    "    parameters=parameters_participant1,\n",
    "    noise_level=0,  # No noise for now, so we can see the pure parameter effect\n",
    ")\n",
    "\n",
    "participant2 = SyntheticParticipant(\n",
    "    cognitive_model=cognitive_model,\n",
    "    parameters=parameters_participant2,\n",
    "    noise_level=0,\n",
    ")\n",
    "\n",
    "# Define the experimental conditions to test\n",
    "ratio = np.linspace(0, 1)\n",
    "scatteredness = np.linspace(0, 1)\n",
    "ratio_mesh, scatteredness_mesh = np.meshgrid(ratio, scatteredness)\n",
    "sample_size = len(ratio)\n",
    "\n",
    "# Collect observations from participant 1\n",
    "response_time_p1 = np.zeros((sample_size, sample_size))\n",
    "for i in range(sample_size):\n",
    "    response_time_p1[i, :] = participant1.step(ratio_mesh[i], scatteredness_mesh[i], noise=True).reshape(-1)\n",
    "    \n",
    "# Collect observations from participant 2  \n",
    "response_time_p2 = np.zeros((sample_size, sample_size))\n",
    "for i in range(sample_size):\n",
    "    response_time_p2[i, :] = participant2.step(ratio_mesh[i], scatteredness_mesh[i], noise=True).reshape(-1)\n",
    "\n",
    "# Visualize both participants side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": \"3d\"}, figsize=(14, 6))\n",
    "\n",
    "ax1.plot_surface(ratio_mesh, scatteredness_mesh, response_time_p1, cmap=cm.Blues)\n",
    "ax1.set_title(f'Participant 1 (params={parameters_participant1})')\n",
    "ax1.set_xlabel('Ratio')\n",
    "ax1.set_ylabel('Scatteredness')\n",
    "ax1.set_zlabel('Response time')\n",
    "\n",
    "ax2.plot_surface(ratio_mesh, scatteredness_mesh, response_time_p2, cmap=cm.Greens)\n",
    "ax2.set_title(f'Participant 2 (params={parameters_participant2})')\n",
    "ax2.set_xlabel('Ratio')\n",
    "ax2.set_ylabel('Scatteredness')\n",
    "ax2.set_zlabel('Response time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a dataset from multiple synthetic participants\n",
    "\n",
    "Now we'll generate a complete dataset by \"testing\" multiple synthetic participants on various experimental conditions. This simulates running a real experiment with many participants.\n",
    "\n",
    "The dataset will have:\n",
    "- **Between-participant variability**: Different participants have different parameters\n",
    "- **Measurement noise**: Each observation includes random noise\n",
    "\n",
    "We'll use a pre-defined function from `resources.synthetic` to generate the dataset efficiently.\n",
    "\n",
    "In this section you will learn to:\n",
    "- Define the study parameters (number of participants, number of conditions tested)\n",
    "- Generate diverse synthetic participants with varying parameters\n",
    "- Create a dataset by testing all participants on randomly sampled conditions\n",
    "- Understand the structure of the resulting dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define the parameters for our simulated study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset parameters\n",
    "\n",
    "# Number of synthetic participants to create\n",
    "n_participants = 100\n",
    "\n",
    "# Number of different conditions each participant will be tested on\n",
    "n_conditions = 100\n",
    "\n",
    "# Measurement noise level\n",
    "noise_level = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll generate diverse participant parameters and experimental conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate participant parameters\n",
    "# We draw parameters from a normal distribution around 1, creating individual differences\n",
    "# Parameters are constrained to be positive (can't have negative sensitivity)\n",
    "parameters = np.random.normal(1, 0.5, (n_participants, 2))\n",
    "parameters = np.where(parameters < 0, 0.1, parameters)  # Replace any negative values with 0.1\n",
    "parameters = np.where(parameters > 2, 1.9, parameters)\n",
    "\n",
    "# Generate experimental conditions\n",
    "# We sample random combinations of ratio and scatteredness\n",
    "conditions = np.random.uniform(0, 1, (n_conditions, 2))\n",
    "\n",
    "print(f\"Created {n_participants} synthetic participants\")\n",
    "print(f\"Generated {n_conditions} experimental conditions\")\n",
    "print(f\"Example participant parameters: {parameters[0]}\")\n",
    "print(f\"Example condition: ratio={conditions[0, 0]:.3f}, scatteredness={conditions[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create all synthetic participants and collect data from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset generation function\n",
    "from resources.synthetic import generate_dataset\n",
    "\n",
    "# Create synthetic participants\n",
    "synthetic_participants = []\n",
    "for i in range(n_participants):\n",
    "    synthetic_participants.append(\n",
    "        SyntheticParticipant(\n",
    "            cognitive_model=cognitive_model,\n",
    "            parameters=parameters[i],\n",
    "            noise_level=noise_level,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Generate the dataset\n",
    "# This tests all participants on all conditions and splits into train/test\n",
    "dataset_train, dataset_test = generate_dataset(\n",
    "    experimental_units=synthetic_participants,\n",
    "    conditions=conditions,\n",
    "    train_ratio=0.8,  # 80% for training, 20% for testing\n",
    ")\n",
    "\n",
    "print(f\"Training dataset shape: {dataset_train.shape}\")\n",
    "print(f\"Test dataset shape: {dataset_test.shape}\")\n",
    "print(f\"Columns: [participant_id, ratio, scatteredness, response_time]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the dataset and visualize how response times vary across participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows of the dataset\n",
    "print(\"First 5 rows of training data:\")\n",
    "print(dataset_train[:5])\n",
    "print()\n",
    "\n",
    "# Compute participant-level averages\n",
    "participant_means = []\n",
    "participant_stds = []\n",
    "for p_id in range(n_participants):\n",
    "    p_data = dataset_train[dataset_train[:, 0] == p_id, -1]\n",
    "    if len(p_data) > 0:\n",
    "        participant_means.append(p_data.mean())\n",
    "        participant_stds.append(p_data.std())\n",
    "\n",
    "# Visualize between-participant variability\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot mean response time per participant\n",
    "ax1.bar(range(len(participant_means)), participant_means, alpha=0.7)\n",
    "ax1.set_xlabel('Participant ID')\n",
    "ax1.set_ylabel('Mean response time')\n",
    "ax1.set_title('Average response time by participant')\n",
    "\n",
    "# Plot within-participant variability\n",
    "ax2.bar(range(len(participant_stds)), participant_stds, alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Participant ID')\n",
    "ax2.set_ylabel('Std of response time')\n",
    "ax2.set_title('Response time variability by participant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see variability both between and within participants. The between-participant variability comes from different parameter values, while the within-participant variability comes from measurement noise.\n",
    "\n",
    "How would you expect the variability to change if you increased the number of conditions tested or increased the noise level? Test your intuition by re-running the dataset generation with different values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting\n",
    "\n",
    "In this section we'll fit statistical models to our synthetic dataset to recover the underlying cognitive model parameters. This demonstrates a key use of synthetic experiments: we know the true parameters, so we can test how well our analysis methods work.\n",
    "\n",
    "We'll fit two types of models:\n",
    "1. **Linear regression**: A simple group-level model that estimates average effects across all participants\n",
    "2. **Neural network**: A more flexible model that can capture individual differences and non-linear patterns\n",
    "\n",
    "In this section you will learn to:\n",
    "- Prepare data for model fitting (the data is already split into train/test)\n",
    "- Fit models to recover cognitive parameters from behavioral data\n",
    "- Evaluate how well the models recover the true underlying patterns\n",
    "- Understand how sample size and noise affect parameter recovery quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The dataset is already split into training and test sets by the `generate_dataset` function:\n",
    "- **Training data**: Used to fit the model parameters\n",
    "- **Test data**: Used to evaluate how well the model generalizes to new conditions\n",
    "\n",
    "Let's check what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training samples: {len(dataset_train)}')\n",
    "print(f'Number of test samples: {len(dataset_test)}')\n",
    "print(f'Dataset structure: [participant_id, ratio, scatteredness, response_time]')\n",
    "print(f'\\nExample training samples:')\n",
    "print(dataset_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a linear regression model\n",
    "\n",
    "We'll use a simple linear regression model from sklearn. This model estimates group-level effects - the average influence of ratio and scatteredness across all participants.\n",
    "\n",
    "**Note**: Linear regression won't capture the non-linear pattern in our cognitive model perfectly, but it gives us a baseline to see how well a simple model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "# We use columns 1:3 (ratio and scatteredness) to predict column 3 (response time)\n",
    "# We ignore participant_id since linear regression doesn't model individual differences\n",
    "model.fit(X=dataset_train[:, 1:3], y=dataset_train[:, 3])\n",
    "\n",
    "# Extract the fitted coefficients\n",
    "coef_ratio = model.coef_[0]\n",
    "coef_scatteredness = model.coef_[1]\n",
    "\n",
    "# Compare to true average parameters\n",
    "true_params_mean = np.mean(parameters, axis=0)\n",
    "print('Parameter Recovery:')\n",
    "print(f'True average parameters: ratio_sensitivity={true_params_mean[0]:.3f}, scatteredness_sensitivity={true_params_mean[1]:.3f}')\n",
    "print(f'Linear model coefficients: ratio={coef_ratio:.3f}, scatteredness={coef_scatteredness:.3f}')\n",
    "print(f'\\nNote: Linear regression coefficients are not directly comparable to cognitive model parameters')\n",
    "print(f'because our cognitive model is non-linear. But they give us a sense of the trends.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the linear model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Use the fitted model to predict test observations\n",
    "predictions = model.predict(dataset_test[:, 1:3])\n",
    "\n",
    "# Compute prediction metrics\n",
    "mse = mean_squared_error(dataset_test[:, 3], predictions)\n",
    "r2 = r2_score(dataset_test[:, 3], predictions)\n",
    "\n",
    "print(f'Test set performance:')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "print(f'\\nOriginal mean response time: {np.mean(dataset_test[:, 3]):.3f}')\n",
    "print(f'Predicted mean response time: {np.mean(predictions):.3f}')\n",
    "\n",
    "# Visualize the fitted model's predictions\n",
    "ratio = np.linspace(0, 1, 50)\n",
    "scatteredness = np.linspace(0, 1, 50)\n",
    "ratio_mesh, scatteredness_mesh = np.meshgrid(ratio, scatteredness)\n",
    "sample_size = len(ratio)\n",
    "\n",
    "# Get model predictions\n",
    "response_time_predicted = np.zeros((sample_size, sample_size))\n",
    "for i in range(sample_size):\n",
    "    conditions = np.stack((ratio_mesh[i], scatteredness_mesh[i]), axis=-1)\n",
    "    response_time_predicted[i, :] = model.predict(conditions)\n",
    "\n",
    "# Plot the linear model's predictions\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "ax.plot_surface(ratio_mesh, scatteredness_mesh, response_time_predicted, cmap=cm.Reds, alpha=0.7)\n",
    "ax.set_title('Linear Model Predictions')\n",
    "ax.set_xlabel('Ratio')\n",
    "ax.set_ylabel('Scatteredness')\n",
    "ax.set_zlabel('Response time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R² score tells you what proportion of variance in response times the model explains (1.0 = perfect, 0.0 = no better than guessing the mean).\n",
    "\n",
    "The linear model approximates the overall trend but can't capture the non-linear pattern perfectly. Try adjusting the number of participants, conditions, or noise level to see how it affects model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a neural network\n",
    "\n",
    "Now let's fit a more flexible neural network model. Unlike linear regression, the neural network can:\n",
    "- Learn non-linear patterns in the data\n",
    "- Model individual differences between participants (by using participant_id as input)\n",
    "\n",
    "We'll use a pre-defined feed-forward network from `resources.regressors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.regressors import FFN, FFNRegressor\n",
    "\n",
    "# Train a neural network model\n",
    "# The network takes [participant_id, ratio, scatteredness] as input\n",
    "model_ffn = FFNRegressor(FFN(n_participants, 2), max_epochs=100, lr=0.1)\n",
    "model_ffn.fit(dataset_train[:, 0:3], dataset_train[:, 3:4].astype(np.float32))\n",
    "\n",
    "# Get predictions on the test data\n",
    "predictions_ffn = model_ffn.predict(dataset_test[:, 0:3])\n",
    "mse_ffn = mean_squared_error(dataset_test[:, 3], predictions_ffn)\n",
    "r2_ffn = r2_score(dataset_test[:, 3], predictions_ffn)\n",
    "\n",
    "print(f\"Neural Network Test Performance:\")\n",
    "print(f\"Mean Squared Error: {mse_ffn:.4f}\")\n",
    "print(f\"R² Score: {r2_ffn:.4f}\")\n",
    "print(f\"\\nComparison to Linear Regression:\")\n",
    "print(f\"Linear MSE: {mse:.4f} vs Neural Net MSE: {mse_ffn:.4f}\")\n",
    "print(f\"Linear R²: {r2:.4f} vs Neural Net R²: {r2_ffn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the neural network compare to linear regression? \n",
    "\n",
    "The neural network should perform better (lower MSE, higher R²) because it can learn the non-linear cognitive model pattern and individual differences. However, it needs more data to train effectively.\n",
    "\n",
    "Let's visualize how well the neural network recovers individual participant behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neural network predictions for a specific participant\n",
    "participant_id = 1\n",
    "\n",
    "# Define conditions to test\n",
    "ratio = np.linspace(0, 1, 50)\n",
    "scatteredness = np.linspace(0, 1, 50)\n",
    "ratio_mesh, scatteredness_mesh = np.meshgrid(ratio, scatteredness)\n",
    "sample_size = len(ratio)\n",
    "\n",
    "# Get neural network predictions for this participant\n",
    "response_time_nn = np.zeros((sample_size, sample_size))\n",
    "for i in range(sample_size):\n",
    "    conditions = np.stack((ratio_mesh[i], scatteredness_mesh[i]), axis=-1)\n",
    "    participant_id_array = np.full((conditions.shape[0], 1), participant_id)\n",
    "    X = np.concatenate((participant_id_array, conditions), axis=-1)\n",
    "    response_time_nn[i, :] = model_ffn.predict(X).reshape(-1)\n",
    "\n",
    "# Get true response times from the cognitive model\n",
    "response_time_true = np.zeros((sample_size, sample_size))\n",
    "for i in range(sample_size):\n",
    "    response_time_true[i, :] = synthetic_participants[participant_id].step(ratio_mesh[i], scatteredness_mesh[i], noise=False).reshape(-1)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1) = plt.subplots(1, 1, subplot_kw={\"projection\": \"3d\"}, figsize=(14, 6))\n",
    "\n",
    "# True cognitive model\n",
    "ax1.plot_surface(ratio_mesh, scatteredness_mesh, response_time_true, cmap=cm.Blues, alpha=0.7)\n",
    "ax1.plot_surface(ratio_mesh, scatteredness_mesh, response_time_nn, cmap=cm.Reds, alpha=0.7)\n",
    "ax1.set_title(f'True Cognitive Model vs Neural Network Prediction (Participant {participant_id})')\n",
    "ax1.set_xlabel('Ratio')\n",
    "ax1.set_ylabel('Scatteredness')\n",
    "ax1.set_zlabel('Response time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"You can change 'participant_id' to visualize different participants (0 to {n_participants-1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You've completed the first tutorial on synthetic experimentation!\n",
    "\n",
    "You should now have a good understanding of:\n",
    "- How to implement a cognitive model to simulate participant behavior\n",
    "- How model parameters represent individual differences between participants\n",
    "- How measurement noise affects experimental data\n",
    "- How to generate synthetic datasets from multiple participants\n",
    "- How to fit and evaluate statistical models to recover cognitive parameters\n",
    "- How sample size and noise level impact parameter recovery quality\n",
    "\n",
    "In the next tutorials, you'll learn how to optimize experimental designs using these synthetic experiments to test different experimental strategies before running real studies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
