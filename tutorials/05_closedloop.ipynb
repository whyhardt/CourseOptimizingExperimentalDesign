{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-loop experimentation with active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you'll learn how to implement a complete closed-loop experiment using the AutoRA framework. \n",
    "\n",
    "In traditional active learning, we typically have a fixed pool of unlabeled data (e.g., images) that we want to label efficiently by selecting the most informative samples. In experimental design, however, we don't have a pre-existing pool of conditions - instead, we can **generate new experimental conditions on the fly** and collect observations from participants.\n",
    "\n",
    "This leads us to **closed-loop experimentation**, where:\n",
    "1. An **experimentalist** proposes new experimental conditions\n",
    "2. An **experiment runner** collects observations at those conditions\n",
    "3. A **theorist** (model) learns from the accumulated data\n",
    "4. The cycle repeats, with the experimentalist using the current model to propose even better conditions\n",
    "\n",
    "The AutoRA framework connects these three components:\n",
    "\n",
    "![static/img/autora_workflow.png](static/img/autora_workflow.png)\n",
    "\n",
    "(Image source: [autora](https://autoresearch.github.io/autora/))\n",
    "\n",
    "In this tutorial, you will learn to:\n",
    "- Set up the AutoRA experiment runner for the 2AFC task\n",
    "- Implement a cyclic for-loop for closed-loop experimentation\n",
    "- Collect data iteratively and fit models at each cycle\n",
    "- Track model performance across cycles\n",
    "- Understand how to integrate different experimentalist strategies\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the project folder\n",
    "target_folder = os.path.abspath(os.path.join(os.getcwd(), '..'))  # Adjust path as needed\n",
    "if target_folder not in sys.path:\n",
    "    sys.path.append(target_folder)\n",
    "    \n",
    "from resources.regressors import FFN, FFNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the synthetic experiment\n",
    "\n",
    "First, let's set up our synthetic 2AFC experiment with multiple synthetic participants. Each participant has their own cognitive model parameters that determine how they respond to different experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic experiment parameters\n",
    "\n",
    "n_units = 100  # Number of synthetic participants\n",
    "noise_level = 0.3  # Measurement noise level\n",
    "\n",
    "# Sample parameters for each synthetic participant\n",
    "# Each participant has 2 parameters controlling their sensitivity to ratio and scatteredness\n",
    "parameters = np.random.normal(1, 0.5, (n_units, 2))\n",
    "parameters = np.where(parameters < 0, 0.1, parameters)  # Ensure positive parameters\n",
    "\n",
    "print(f\"Created {n_units} synthetic participants with individual cognitive parameters\")\n",
    "print(f\"Example participant parameters: {parameters[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the AutoRA experiment runner\n",
    "\n",
    "The AutoRA experiment runner is a wrapper around our 2AFC experiment that:\n",
    "- Defines the **independent variables** (conditions we can control): participant_id, ratio, scatteredness\n",
    "- Defines the **dependent variables** (observations we measure): response_time\n",
    "- Provides a standardized interface for running experiments\n",
    "\n",
    "Think of it as a way to package our experiment so it can be easily integrated into the closed-loop workflow.\n",
    "\n",
    "Let's import and explore the experiment runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resources.synthetic import twoafc\n",
    "\n",
    "# Check the help documentation\n",
    "help(twoafc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experiment_runner.synthetic.utilities import describe\n",
    "\n",
    "# Create the experiment runner with our synthetic participants\n",
    "experiment = twoafc(parameters=parameters, noise_level=noise_level)\n",
    "\n",
    "# Describe the experiment structure\n",
    "print(describe(experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment runner has two key methods:\n",
    "- `experiment.run(conditions)`: Runs the experiment on specified conditions and returns observations\n",
    "- `experiment.plotter()`: Visualizes the ground truth cognitive model and fitted model predictions\n",
    "\n",
    "Let's test it by running a single condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment on a single condition: [participant_id=0, ratio=1, scatteredness=1]\n",
    "result = experiment.run(np.array([[1, 1]]), random_state=42)\n",
    "print(\"Experiment result:\")\n",
    "print(result)\n",
    "\n",
    "# Visualize the underlying ground truth for participant 0\n",
    "experiment.plotter(participant_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the closed-loop components\n",
    "\n",
    "Now let's set up the three key components of our closed-loop experiment:\n",
    "\n",
    "1. **Experiment runner**: We've already created this above (`experiment`)\n",
    "2. **Experimentalist**: Proposes new experimental conditions to test\n",
    "3. **Theorist**: A machine learning model that learns from the data\n",
    "\n",
    "For the experimentalist, we'll start with a simple **random sampling** strategy that uniformly samples conditions from the experimental space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.random import random_pool\n",
    "\n",
    "# Define the experimentalist (random sampling strategy)\n",
    "experimentalist = random_pool\n",
    "\n",
    "# Define the theorist (neural network model)\n",
    "theorist = FFNRegressor(\n",
    "    FFN(n_units=n_units, n_conditions=2),  # n_conditions=2 for ratio and scatteredness\n",
    "    max_epochs=100,\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "print(\"Closed-loop components initialized:\")\n",
    "print(f\"  Experiment: 2AFC task with {n_units} participants\")\n",
    "print(f\"  Experimentalist: Random sampling\")\n",
    "print(f\"  Theorist: Feed-forward neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the variable structure\n",
    "\n",
    "Before we start the loop, let's understand how the experiment variables are structured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract variable names from the experiment\n",
    "iv_names = [iv.name for iv in experiment.variables.independent_variables]\n",
    "dv_names = [dv.name for dv in experiment.variables.dependent_variables]\n",
    "\n",
    "print(\"Independent variables (conditions):\", iv_names)\n",
    "print(\"Dependent variables (observations):\", dv_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating initial conditions\n",
    "\n",
    "Every closed-loop experiment needs some initial conditions to start with. We'll use the experimentalist to generate an initial set of random conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial random conditions\n",
    "n_samples = 100  # Number of conditions to sample per cycle\n",
    "\n",
    "conditions = experimentalist(experiment.variables, n_samples).to_numpy()\n",
    "\n",
    "print(f\"Generated {len(conditions)} initial conditions\")\n",
    "print(f\"Condition shape: {conditions.shape}\")\n",
    "print(f\"First few conditions:\")\n",
    "print(conditions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The closed-loop experimentation cycle\n",
    "\n",
    "Now we're ready to implement the main closed-loop! \n",
    "\n",
    "Here's how each cycle works:\n",
    "\n",
    "1. **Run experiment**: Collect observations at the current conditions\n",
    "2. **Split data**: Divide into training and test sets\n",
    "3. **Fit model**: Train the theorist on the training data\n",
    "4. **Evaluate**: Test the model on held-out data\n",
    "5. **Generate new conditions**: Use the experimentalist to propose new conditions\n",
    "6. **Accumulate**: Add new conditions to the pool for the next cycle\n",
    "\n",
    "This creates a **cyclic for-loop** where we continuously improve our model by collecting more data and refining our understanding.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed-loop parameters\n",
    "n_cycles = 1  # Number of cycles to run\n",
    "train_ratio = 1.0  # Proportion of data to use for training (80% train, 20% test)\n",
    "\n",
    "# Start the closed-loop experimentation cycle\n",
    "for cycle in range(n_cycles):\n",
    "    \n",
    "    # Step 1: Run the experiment on current conditions\n",
    "    experiment_data = experiment.run(conditions, shuffle=True).to_numpy()\n",
    "    \n",
    "    # Step 3: Fit the theorist on training data\n",
    "    n_dv = len(experiment.variables.dependent_variables)\n",
    "    X_train = experiment_data[:, :-n_dv]\n",
    "    y_train = experiment_data[:, -n_dv:]\n",
    "    \n",
    "    theorist.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    # Step 4: Evaluate the theorist on test data\n",
    "    conditions_test = experimentalist(experiment.variables, num_samples=10000)\n",
    "    experiment_data_test = experiment.run(conditions_test, noise_level_run=0).to_numpy()\n",
    "    X_test = experiment_data_test[:, :-n_dv]\n",
    "    y_test = experiment_data_test[:, -n_dv:]\n",
    "    \n",
    "    y_predicted = theorist.predict(X_test)\n",
    "    mse = np.mean((y_test - y_predicted)**2)\n",
    "    \n",
    "    # Step 5: Generate new conditions for the next cycle\n",
    "    new_conditions = experimentalist(experiment.variables, n_samples).to_numpy()\n",
    "    \n",
    "    # Step 6: Accumulate conditions for the next cycle\n",
    "    conditions = np.concatenate((conditions, new_conditions))\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Cycle {cycle+1}/{n_cycles}: Test MSE = {np.round(mse, 8)}\")\n",
    "\n",
    "print(f\"\\nClosed-loop completed! Total conditions tested: {len(conditions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "\n",
    "Now let's visualize how well our trained model recovered the underlying cognitive model for a specific participant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ground truth and model predictions for participant 0\n",
    "experiment.plotter(\n",
    "    participant_id=0,\n",
    "    model=theorist,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows:\n",
    "- **Blue surface**: The true underlying cognitive model\n",
    "- **Red surface**: The neural network's learned predictions\n",
    "\n",
    "How well does the model capture the true pattern? Try:\n",
    "- Increasing `n_cycles` to see if more cycles improve the fit\n",
    "- Changing `n_samples` to collect more/fewer conditions per cycle\n",
    "- Adding noise by setting `noise_level > 0` at the beginning\n",
    "- Testing different participants by changing `participant_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the cyclic for-loop\n",
    "\n",
    "The key advantage of this cyclic approach is its simplicity and transparency:\n",
    "\n",
    "```python\n",
    "for cycle in range(n_cycles):\n",
    "    1. Run experiment on current conditions\n",
    "    2. Split into train/test\n",
    "    3. Fit model on training data\n",
    "    4. Evaluate on test data\n",
    "    5. Generate new conditions\n",
    "    6. Accumulate conditions\n",
    "```\n",
    "\n",
    "Each step is explicit and easy to understand. You can:\n",
    "- Add custom logging or visualization at any step\n",
    "- Modify the data processing pipeline\n",
    "- Integrate different experimentalist strategies\n",
    "- Track additional metrics\n",
    "\n",
    "This makes it perfect for learning and experimentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different experimentalists\n",
    "\n",
    "So far, we've used random sampling to propose new conditions. But AutoRA provides also other experimentalists like the **grid sampler**.\n",
    "\n",
    "You can install them with:\n",
    "```bash\n",
    "pip install -U \"autora[all-experimentalists]\"\n",
    "```\n",
    "\n",
    "Or install specific ones:\n",
    "```bash\n",
    "pip install -U \"autora[experimentalist-grid]\"\n",
    "```\n",
    "\n",
    "Let's try using the **grid sampler** instead of random sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.grid import grid_pool\n",
    "\n",
    "# Reset the experiment and theorist\n",
    "experiment = twoafc(parameters, noise_level=noise_level, discrete_iv=True, resolution=10)\n",
    "theorist = FFNRegressor(FFN(n_units=n_units, n_conditions=2), max_epochs=100, lr=0.1)\n",
    "\n",
    "# Use grid sampling instead of random sampling\n",
    "experimentalist = grid_pool\n",
    "experimentalist_random = random_pool\n",
    "\n",
    "# Generate initial grid conditions\n",
    "conditions = experimentalist(experiment.variables).to_numpy()\n",
    "\n",
    "print(f\"Using grid sampling strategy\")\n",
    "print(f\"First few grid conditions:\")\n",
    "print(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same closed-loop with grid sampling\n",
    "for cycle in range(n_cycles):\n",
    "    experiment_data = experiment.run(conditions, shuffle=True).to_numpy()\n",
    "    \n",
    "    n_dv = len(experiment.variables.dependent_variables)\n",
    "    X_train = experiment_data[:, :-n_dv]\n",
    "    y_train = experiment_data[:, -n_dv:]\n",
    "    \n",
    "    theorist.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    conditions_test = experimentalist_random(experiment.variables, num_samples=10000)\n",
    "    experiment_data_test = experiment.run(conditions_test, noise_level_run=0).to_numpy()\n",
    "    X_test = experiment_data_test[:, :-n_dv]\n",
    "    y_test = experiment_data_test[:, -n_dv:]\n",
    "    \n",
    "    y_predicted = theorist.predict(X_test)\n",
    "    mse = np.mean((y_test - y_predicted)**2)\n",
    "    \n",
    "    new_conditions = experimentalist(experiment.variables).to_numpy()\n",
    "    conditions = np.concatenate((conditions, new_conditions))\n",
    "    \n",
    "    print(f\"Cycle {cycle+1}/{n_cycles} [Grid Sampling]: Test MSE = {np.round(mse, 8)}\")\n",
    "\n",
    "# Visualize results\n",
    "experiment.plotter(participant_id=0, model=theorist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice any differences between random and grid sampling?\n",
    "\n",
    "Grid sampling provides more systematic coverage of the experimental space, which can be beneficial when you want to ensure even sampling across all regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You have completed all tutorials in the **Optimizing Experimental Design Course**! \n",
    "\n",
    "You've learned:\n",
    "- How to set up synthetic experiments with cognitive models\n",
    "- How to implement closed-loop experimentation with AutoRA\n",
    "- How to integrate different experimentalist strategies\n",
    "- How to evaluate model performance across cycles\n",
    "\n",
    "AutoRA comes with many more sophisticated experimentalists:\n",
    "\n",
    "- **Random**: Uniform random sampling (what we used)\n",
    "- **Grid**: Systematic grid sampling\n",
    "- **Novelty**: Samples conditions far from previous conditions\n",
    "- **Inequality**: Samples where model predictions vary most\n",
    "- **Nearest Value**: Samples near specific target values\n",
    "- **Disagreement**: Samples where ensemble models disagree (requires ensemble)\n",
    "- **Uncertainty**: Samples where model is most uncertain (requires ensemble)\n",
    "- **Leverage**: Samples with high influence on model parameters\n",
    "- **Falsification**: Samples likely to falsify current theory\n",
    "- **Mixture**: Combines multiple strategies\n",
    "\n",
    "In the next tutorials we are going to learn more about the **Uncertainty** and **Disagreement samplers**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
